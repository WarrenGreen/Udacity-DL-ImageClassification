{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:36, 4.69MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116c5a748>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    out = np.zeros((len(x), 10))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        out[i][x[i]] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, (None, image_shape[0], image_shape[1], image_shape[2]), name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.int32, (None, n_classes), name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[-1], conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    cell = tf.nn.conv2d(x_tensor, filter=weight, strides=[1,conv_strides[0],conv_strides[1],1], padding=\"SAME\")\n",
    "    cell = tf.nn.bias_add(cell, bias)\n",
    "    cell = tf.nn.relu(cell)\n",
    "    cell = tf.nn.max_pool(cell, [1,pool_ksize[0], pool_ksize[1],1], [1,pool_strides[0],pool_strides[1],1], padding=\"SAME\")\n",
    "    return cell \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    hidden_layer = tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=tf.nn.relu)\n",
    "    #weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs]))\n",
    "    #bias = tf.Variable(tf.truncated_normal([num_outputs]))\n",
    "    #hidden_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    return hidden_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs]))\n",
    "    #bias = tf.Variable(tf.truncated_normal([num_outputs]))\n",
    "    #output_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    output_layer = tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    #  Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #   conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    cell = conv2d_maxpool(x, 8, (4,4), (2,2), (2,2), (2,2))\n",
    "    cell = conv2d_maxpool(cell, 16, (4,4), (2,2), (2,2), (2,2))\n",
    "    cell = conv2d_maxpool(cell, 32, (4,4), (2,2), (2,2), (2,2))\n",
    "\n",
    "    #  Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    cell = flatten(cell)\n",
    "    \n",
    "\n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    cell = fully_conn(cell, 128)\n",
    "    cell = tf.nn.dropout(cell, keep_prob)\n",
    "    cell = fully_conn(cell, 64)\n",
    "    #  Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    cell= output(cell, 10)\n",
    "    # return output\n",
    "    return cell\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    feed = {x: valid_features, y: valid_labels, keep_prob:1.0}\n",
    "    acc = session.run(accuracy, feed_dict=feed)\n",
    "    feed = {x: feature_batch, y: label_batch, keep_prob:1.0}\n",
    "    loss = session.run(cost, feed_dict=feed)\n",
    "    print(\"Accuracy: {}\".format(acc), \"Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "keep_probability = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Accuracy: 0.14659999310970306 Loss: 2.7569031715393066\n",
      "Epoch  2, CIFAR-10 Batch 1:  Accuracy: 0.15839999914169312 Loss: 2.377568006515503\n",
      "Epoch  3, CIFAR-10 Batch 1:  Accuracy: 0.15279999375343323 Loss: 2.2536587715148926\n",
      "Epoch  4, CIFAR-10 Batch 1:  Accuracy: 0.15440000593662262 Loss: 2.2628073692321777\n",
      "Epoch  5, CIFAR-10 Batch 1:  Accuracy: 0.1746000051498413 Loss: 2.272726535797119\n",
      "Epoch  6, CIFAR-10 Batch 1:  Accuracy: 0.18539999425411224 Loss: 2.237204074859619\n",
      "Epoch  7, CIFAR-10 Batch 1:  Accuracy: 0.1842000037431717 Loss: 2.1894140243530273\n",
      "Epoch  8, CIFAR-10 Batch 1:  Accuracy: 0.1868000030517578 Loss: 2.1770718097686768\n",
      "Epoch  9, CIFAR-10 Batch 1:  Accuracy: 0.20800000429153442 Loss: 2.12996244430542\n",
      "Epoch 10, CIFAR-10 Batch 1:  Accuracy: 0.2134000062942505 Loss: 2.1030304431915283\n",
      "Epoch 11, CIFAR-10 Batch 1:  Accuracy: 0.21780000627040863 Loss: 2.072838544845581\n",
      "Epoch 12, CIFAR-10 Batch 1:  Accuracy: 0.2272000014781952 Loss: 2.067729949951172\n",
      "Epoch 13, CIFAR-10 Batch 1:  Accuracy: 0.22939999401569366 Loss: 2.032379388809204\n",
      "Epoch 14, CIFAR-10 Batch 1:  Accuracy: 0.24120000004768372 Loss: 2.003166913986206\n",
      "Epoch 15, CIFAR-10 Batch 1:  Accuracy: 0.2409999966621399 Loss: 1.9542710781097412\n",
      "Epoch 16, CIFAR-10 Batch 1:  Accuracy: 0.24480000138282776 Loss: 1.920998215675354\n",
      "Epoch 17, CIFAR-10 Batch 1:  Accuracy: 0.2542000114917755 Loss: 1.883935570716858\n",
      "Epoch 18, CIFAR-10 Batch 1:  Accuracy: 0.2502000033855438 Loss: 1.869055151939392\n",
      "Epoch 19, CIFAR-10 Batch 1:  Accuracy: 0.257999986410141 Loss: 1.839634656906128\n",
      "Epoch 20, CIFAR-10 Batch 1:  Accuracy: 0.2712000012397766 Loss: 1.7844709157943726\n",
      "Epoch 21, CIFAR-10 Batch 1:  Accuracy: 0.2847999930381775 Loss: 1.714098334312439\n",
      "Epoch 22, CIFAR-10 Batch 1:  Accuracy: 0.2865999937057495 Loss: 1.6642967462539673\n",
      "Epoch 23, CIFAR-10 Batch 1:  Accuracy: 0.2800000011920929 Loss: 1.6527116298675537\n",
      "Epoch 24, CIFAR-10 Batch 1:  Accuracy: 0.28459998965263367 Loss: 1.6246488094329834\n",
      "Epoch 25, CIFAR-10 Batch 1:  Accuracy: 0.28619998693466187 Loss: 1.5843111276626587\n",
      "Epoch 26, CIFAR-10 Batch 1:  Accuracy: 0.29280000925064087 Loss: 1.550921082496643\n",
      "Epoch 27, CIFAR-10 Batch 1:  Accuracy: 0.29660001397132874 Loss: 1.5165460109710693\n",
      "Epoch 28, CIFAR-10 Batch 1:  Accuracy: 0.3003999888896942 Loss: 1.4904563426971436\n",
      "Epoch 29, CIFAR-10 Batch 1:  Accuracy: 0.30379998683929443 Loss: 1.478758692741394\n",
      "Epoch 30, CIFAR-10 Batch 1:  Accuracy: 0.2996000051498413 Loss: 1.4535667896270752\n",
      "Epoch 31, CIFAR-10 Batch 1:  Accuracy: 0.3057999908924103 Loss: 1.4479535818099976\n",
      "Epoch 32, CIFAR-10 Batch 1:  Accuracy: 0.30300000309944153 Loss: 1.3875617980957031\n",
      "Epoch 33, CIFAR-10 Batch 1:  Accuracy: 0.30399999022483826 Loss: 1.3946831226348877\n",
      "Epoch 34, CIFAR-10 Batch 1:  Accuracy: 0.3070000112056732 Loss: 1.361318588256836\n",
      "Epoch 35, CIFAR-10 Batch 1:  Accuracy: 0.3070000112056732 Loss: 1.3474411964416504\n",
      "Epoch 36, CIFAR-10 Batch 1:  Accuracy: 0.30320000648498535 Loss: 1.3274486064910889\n",
      "Epoch 37, CIFAR-10 Batch 1:  Accuracy: 0.3057999908924103 Loss: 1.278734803199768\n",
      "Epoch 38, CIFAR-10 Batch 1:  Accuracy: 0.31839999556541443 Loss: 1.2606836557388306\n",
      "Epoch 39, CIFAR-10 Batch 1:  Accuracy: 0.311599999666214 Loss: 1.2261722087860107\n",
      "Epoch 40, CIFAR-10 Batch 1:  Accuracy: 0.3140000104904175 Loss: 1.2462565898895264\n",
      "Epoch 41, CIFAR-10 Batch 1:  Accuracy: 0.3147999942302704 Loss: 1.2020736932754517\n",
      "Epoch 42, CIFAR-10 Batch 1:  Accuracy: 0.3125999867916107 Loss: 1.1737709045410156\n",
      "Epoch 43, CIFAR-10 Batch 1:  Accuracy: 0.31679999828338623 Loss: 1.1959640979766846\n",
      "Epoch 44, CIFAR-10 Batch 1:  Accuracy: 0.31700000166893005 Loss: 1.1502612829208374\n",
      "Epoch 45, CIFAR-10 Batch 1:  Accuracy: 0.3208000063896179 Loss: 1.139500379562378\n",
      "Epoch 46, CIFAR-10 Batch 1:  Accuracy: 0.3147999942302704 Loss: 1.123708724975586\n",
      "Epoch 47, CIFAR-10 Batch 1:  Accuracy: 0.3179999887943268 Loss: 1.109043836593628\n",
      "Epoch 48, CIFAR-10 Batch 1:  Accuracy: 0.319599986076355 Loss: 1.0850250720977783\n",
      "Epoch 49, CIFAR-10 Batch 1:  Accuracy: 0.32019999623298645 Loss: 1.0953282117843628\n",
      "Epoch 50, CIFAR-10 Batch 1:  Accuracy: 0.313400000333786 Loss: 1.071703553199768\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Accuracy: 0.13120000064373016 Loss: 3.1292052268981934\n",
      "Epoch  1, CIFAR-10 Batch 2:  Accuracy: 0.11299999803304672 Loss: 2.3344807624816895\n",
      "Epoch  1, CIFAR-10 Batch 3:  Accuracy: 0.0997999981045723 Loss: 2.2972331047058105\n",
      "Epoch  1, CIFAR-10 Batch 4:  Accuracy: 0.09880000352859497 Loss: 2.3012149333953857\n",
      "Epoch  1, CIFAR-10 Batch 5:  Accuracy: 0.09880000352859497 Loss: 2.303920030593872\n",
      "Epoch  2, CIFAR-10 Batch 1:  Accuracy: 0.09780000150203705 Loss: 2.30389142036438\n",
      "Epoch  2, CIFAR-10 Batch 2:  Accuracy: 0.0989999994635582 Loss: 2.30108642578125\n",
      "Epoch  2, CIFAR-10 Batch 3:  Accuracy: 0.10279999673366547 Loss: 2.282651901245117\n",
      "Epoch  2, CIFAR-10 Batch 4:  Accuracy: 0.11819999665021896 Loss: 2.276798725128174\n",
      "Epoch  2, CIFAR-10 Batch 5:  Accuracy: 0.13619999587535858 Loss: 2.287449836730957\n",
      "Epoch  3, CIFAR-10 Batch 1:  Accuracy: 0.1542000025510788 Loss: 2.2380471229553223\n",
      "Epoch  3, CIFAR-10 Batch 2:  Accuracy: 0.15459999442100525 Loss: 2.2761306762695312\n",
      "Epoch  3, CIFAR-10 Batch 3:  Accuracy: 0.15839999914169312 Loss: 2.1654648780822754\n",
      "Epoch  3, CIFAR-10 Batch 4:  Accuracy: 0.15880000591278076 Loss: 2.249772548675537\n",
      "Epoch  3, CIFAR-10 Batch 5:  Accuracy: 0.1648000031709671 Loss: 2.186596393585205\n",
      "Epoch  4, CIFAR-10 Batch 1:  Accuracy: 0.17180000245571136 Loss: 2.236934185028076\n",
      "Epoch  4, CIFAR-10 Batch 2:  Accuracy: 0.17980000376701355 Loss: 2.212395191192627\n",
      "Epoch  4, CIFAR-10 Batch 3:  Accuracy: 0.18619999289512634 Loss: 2.0491342544555664\n",
      "Epoch  4, CIFAR-10 Batch 4:  Accuracy: 0.18379999697208405 Loss: 2.2154364585876465\n",
      "Epoch  4, CIFAR-10 Batch 5:  Accuracy: 0.19280000030994415 Loss: 2.1561203002929688\n",
      "Epoch  5, CIFAR-10 Batch 1:  Accuracy: 0.18780000507831573 Loss: 2.236624240875244\n",
      "Epoch  5, CIFAR-10 Batch 2:  Accuracy: 0.19779999554157257 Loss: 2.18166184425354\n",
      "Epoch  5, CIFAR-10 Batch 3:  Accuracy: 0.19979999959468842 Loss: 1.975213646888733\n",
      "Epoch  5, CIFAR-10 Batch 4:  Accuracy: 0.19660000503063202 Loss: 2.177777051925659\n",
      "Epoch  5, CIFAR-10 Batch 5:  Accuracy: 0.20659999549388885 Loss: 2.131208896636963\n",
      "Epoch  6, CIFAR-10 Batch 1:  Accuracy: 0.2125999927520752 Loss: 2.2080025672912598\n",
      "Epoch  6, CIFAR-10 Batch 2:  Accuracy: 0.2303999960422516 Loss: 2.164433002471924\n",
      "Epoch  6, CIFAR-10 Batch 3:  Accuracy: 0.23479999601840973 Loss: 1.9266221523284912\n",
      "Epoch  6, CIFAR-10 Batch 4:  Accuracy: 0.22679999470710754 Loss: 2.0839617252349854\n",
      "Epoch  6, CIFAR-10 Batch 5:  Accuracy: 0.24719999730587006 Loss: 2.036281108856201\n",
      "Epoch  7, CIFAR-10 Batch 1:  Accuracy: 0.2540000081062317 Loss: 2.194180727005005\n",
      "Epoch  7, CIFAR-10 Batch 2:  Accuracy: 0.2712000012397766 Loss: 2.196303606033325\n",
      "Epoch  7, CIFAR-10 Batch 3:  Accuracy: 0.26080000400543213 Loss: 1.8254423141479492\n",
      "Epoch  7, CIFAR-10 Batch 4:  Accuracy: 0.24959999322891235 Loss: 1.9669997692108154\n",
      "Epoch  7, CIFAR-10 Batch 5:  Accuracy: 0.2727999985218048 Loss: 2.005648136138916\n",
      "Epoch  8, CIFAR-10 Batch 1:  Accuracy: 0.2824000120162964 Loss: 2.1516528129577637\n",
      "Epoch  8, CIFAR-10 Batch 2:  Accuracy: 0.2897999882698059 Loss: 2.121220111846924\n",
      "Epoch  8, CIFAR-10 Batch 3:  Accuracy: 0.28119999170303345 Loss: 1.7715041637420654\n",
      "Epoch  8, CIFAR-10 Batch 4:  Accuracy: 0.28760001063346863 Loss: 1.8912826776504517\n",
      "Epoch  8, CIFAR-10 Batch 5:  Accuracy: 0.28439998626708984 Loss: 1.9721038341522217\n",
      "Epoch  9, CIFAR-10 Batch 1:  Accuracy: 0.29580000042915344 Loss: 2.1284263134002686\n",
      "Epoch  9, CIFAR-10 Batch 2:  Accuracy: 0.31220000982284546 Loss: 2.0870521068573\n",
      "Epoch  9, CIFAR-10 Batch 3:  Accuracy: 0.30160000920295715 Loss: 1.7244060039520264\n",
      "Epoch  9, CIFAR-10 Batch 4:  Accuracy: 0.30720001459121704 Loss: 1.8612890243530273\n",
      "Epoch  9, CIFAR-10 Batch 5:  Accuracy: 0.311599999666214 Loss: 1.9790575504302979\n",
      "Epoch 10, CIFAR-10 Batch 1:  Accuracy: 0.30959999561309814 Loss: 2.0830330848693848\n",
      "Epoch 10, CIFAR-10 Batch 2:  Accuracy: 0.3240000009536743 Loss: 2.056732416152954\n",
      "Epoch 10, CIFAR-10 Batch 3:  Accuracy: 0.30880001187324524 Loss: 1.6686607599258423\n",
      "Epoch 10, CIFAR-10 Batch 4:  Accuracy: 0.3271999955177307 Loss: 1.813941240310669\n",
      "Epoch 10, CIFAR-10 Batch 5:  Accuracy: 0.3255999982357025 Loss: 1.9065818786621094\n",
      "Epoch 11, CIFAR-10 Batch 1:  Accuracy: 0.3310000002384186 Loss: 2.006535053253174\n",
      "Epoch 11, CIFAR-10 Batch 2:  Accuracy: 0.33880001306533813 Loss: 2.0025768280029297\n",
      "Epoch 11, CIFAR-10 Batch 3:  Accuracy: 0.33640000224113464 Loss: 1.6162923574447632\n",
      "Epoch 11, CIFAR-10 Batch 4:  Accuracy: 0.34040001034736633 Loss: 1.7830009460449219\n",
      "Epoch 11, CIFAR-10 Batch 5:  Accuracy: 0.3400000035762787 Loss: 1.8636547327041626\n",
      "Epoch 12, CIFAR-10 Batch 1:  Accuracy: 0.3402000069618225 Loss: 1.9784462451934814\n",
      "Epoch 12, CIFAR-10 Batch 2:  Accuracy: 0.3562000095844269 Loss: 1.9832872152328491\n",
      "Epoch 12, CIFAR-10 Batch 3:  Accuracy: 0.3452000021934509 Loss: 1.5682815313339233\n",
      "Epoch 12, CIFAR-10 Batch 4:  Accuracy: 0.35199999809265137 Loss: 1.7680778503417969\n",
      "Epoch 12, CIFAR-10 Batch 5:  Accuracy: 0.3513999879360199 Loss: 1.8219600915908813\n",
      "Epoch 13, CIFAR-10 Batch 1:  Accuracy: 0.35659998655319214 Loss: 1.9137954711914062\n",
      "Epoch 13, CIFAR-10 Batch 2:  Accuracy: 0.36739999055862427 Loss: 1.9501497745513916\n",
      "Epoch 13, CIFAR-10 Batch 3:  Accuracy: 0.36079999804496765 Loss: 1.5156996250152588\n",
      "Epoch 13, CIFAR-10 Batch 4:  Accuracy: 0.36320000886917114 Loss: 1.7290029525756836\n",
      "Epoch 13, CIFAR-10 Batch 5:  Accuracy: 0.36160001158714294 Loss: 1.7746549844741821\n",
      "Epoch 14, CIFAR-10 Batch 1:  Accuracy: 0.3653999865055084 Loss: 1.8751224279403687\n",
      "Epoch 14, CIFAR-10 Batch 2:  Accuracy: 0.37439998984336853 Loss: 1.9037891626358032\n",
      "Epoch 14, CIFAR-10 Batch 3:  Accuracy: 0.37779998779296875 Loss: 1.4733434915542603\n",
      "Epoch 14, CIFAR-10 Batch 4:  Accuracy: 0.3741999864578247 Loss: 1.6858069896697998\n",
      "Epoch 14, CIFAR-10 Batch 5:  Accuracy: 0.3718000054359436 Loss: 1.738987684249878\n",
      "Epoch 15, CIFAR-10 Batch 1:  Accuracy: 0.37119999527931213 Loss: 1.8753738403320312\n",
      "Epoch 15, CIFAR-10 Batch 2:  Accuracy: 0.3846000134944916 Loss: 1.8738133907318115\n",
      "Epoch 15, CIFAR-10 Batch 3:  Accuracy: 0.3776000142097473 Loss: 1.4667928218841553\n",
      "Epoch 15, CIFAR-10 Batch 4:  Accuracy: 0.37959998846054077 Loss: 1.6683765649795532\n",
      "Epoch 15, CIFAR-10 Batch 5:  Accuracy: 0.3831999897956848 Loss: 1.7234256267547607\n",
      "Epoch 16, CIFAR-10 Batch 1:  Accuracy: 0.3831999897956848 Loss: 1.854657769203186\n",
      "Epoch 16, CIFAR-10 Batch 2:  Accuracy: 0.39079999923706055 Loss: 1.8429698944091797\n",
      "Epoch 16, CIFAR-10 Batch 3:  Accuracy: 0.38920000195503235 Loss: 1.431907296180725\n",
      "Epoch 16, CIFAR-10 Batch 4:  Accuracy: 0.38920000195503235 Loss: 1.65898859500885\n",
      "Epoch 16, CIFAR-10 Batch 5:  Accuracy: 0.3885999917984009 Loss: 1.6719295978546143\n",
      "Epoch 17, CIFAR-10 Batch 1:  Accuracy: 0.39320001006126404 Loss: 1.8390896320343018\n",
      "Epoch 17, CIFAR-10 Batch 2:  Accuracy: 0.39340001344680786 Loss: 1.7938247919082642\n",
      "Epoch 17, CIFAR-10 Batch 3:  Accuracy: 0.39259999990463257 Loss: 1.4265953302383423\n",
      "Epoch 17, CIFAR-10 Batch 4:  Accuracy: 0.3984000086784363 Loss: 1.6399974822998047\n",
      "Epoch 17, CIFAR-10 Batch 5:  Accuracy: 0.3991999924182892 Loss: 1.6511642932891846\n",
      "Epoch 18, CIFAR-10 Batch 1:  Accuracy: 0.3952000141143799 Loss: 1.826115369796753\n",
      "Epoch 18, CIFAR-10 Batch 2:  Accuracy: 0.4020000100135803 Loss: 1.8008067607879639\n",
      "Epoch 18, CIFAR-10 Batch 3:  Accuracy: 0.4016000032424927 Loss: 1.4319236278533936\n",
      "Epoch 18, CIFAR-10 Batch 4:  Accuracy: 0.4052000045776367 Loss: 1.6294978857040405\n",
      "Epoch 18, CIFAR-10 Batch 5:  Accuracy: 0.39899998903274536 Loss: 1.6368262767791748\n",
      "Epoch 19, CIFAR-10 Batch 1:  Accuracy: 0.4032000005245209 Loss: 1.7938133478164673\n",
      "Epoch 19, CIFAR-10 Batch 2:  Accuracy: 0.4023999869823456 Loss: 1.7829021215438843\n",
      "Epoch 19, CIFAR-10 Batch 3:  Accuracy: 0.4083999991416931 Loss: 1.4172207117080688\n",
      "Epoch 19, CIFAR-10 Batch 4:  Accuracy: 0.39739999175071716 Loss: 1.6088749170303345\n",
      "Epoch 19, CIFAR-10 Batch 5:  Accuracy: 0.4047999978065491 Loss: 1.603607177734375\n",
      "Epoch 20, CIFAR-10 Batch 1:  Accuracy: 0.40700000524520874 Loss: 1.726257562637329\n",
      "Epoch 20, CIFAR-10 Batch 2:  Accuracy: 0.4068000018596649 Loss: 1.7258625030517578\n",
      "Epoch 20, CIFAR-10 Batch 3:  Accuracy: 0.41440001130104065 Loss: 1.417433738708496\n",
      "Epoch 20, CIFAR-10 Batch 4:  Accuracy: 0.40619999170303345 Loss: 1.586323857307434\n",
      "Epoch 20, CIFAR-10 Batch 5:  Accuracy: 0.40540000796318054 Loss: 1.5919139385223389\n",
      "Epoch 21, CIFAR-10 Batch 1:  Accuracy: 0.4092000126838684 Loss: 1.7271846532821655\n",
      "Epoch 21, CIFAR-10 Batch 2:  Accuracy: 0.4129999876022339 Loss: 1.713139295578003\n",
      "Epoch 21, CIFAR-10 Batch 3:  Accuracy: 0.4108000099658966 Loss: 1.4043505191802979\n",
      "Epoch 21, CIFAR-10 Batch 4:  Accuracy: 0.40380001068115234 Loss: 1.5546653270721436\n",
      "Epoch 21, CIFAR-10 Batch 5:  Accuracy: 0.4108000099658966 Loss: 1.5836238861083984\n",
      "Epoch 22, CIFAR-10 Batch 1:  Accuracy: 0.40779998898506165 Loss: 1.6666603088378906\n",
      "Epoch 22, CIFAR-10 Batch 2:  Accuracy: 0.4099999964237213 Loss: 1.6863830089569092\n",
      "Epoch 22, CIFAR-10 Batch 3:  Accuracy: 0.41999998688697815 Loss: 1.422829508781433\n",
      "Epoch 22, CIFAR-10 Batch 4:  Accuracy: 0.41760000586509705 Loss: 1.5752155780792236\n",
      "Epoch 22, CIFAR-10 Batch 5:  Accuracy: 0.40939998626708984 Loss: 1.5688371658325195\n",
      "Epoch 23, CIFAR-10 Batch 1:  Accuracy: 0.41600000858306885 Loss: 1.6381393671035767\n",
      "Epoch 23, CIFAR-10 Batch 2:  Accuracy: 0.41339999437332153 Loss: 1.7007659673690796\n",
      "Epoch 23, CIFAR-10 Batch 3:  Accuracy: 0.421999990940094 Loss: 1.4046533107757568\n",
      "Epoch 23, CIFAR-10 Batch 4:  Accuracy: 0.41920000314712524 Loss: 1.56903874874115\n",
      "Epoch 23, CIFAR-10 Batch 5:  Accuracy: 0.4108000099658966 Loss: 1.5539801120758057\n",
      "Epoch 24, CIFAR-10 Batch 1:  Accuracy: 0.4146000146865845 Loss: 1.597420573234558\n",
      "Epoch 24, CIFAR-10 Batch 2:  Accuracy: 0.42179998755455017 Loss: 1.662274956703186\n",
      "Epoch 24, CIFAR-10 Batch 3:  Accuracy: 0.42239999771118164 Loss: 1.4324557781219482\n",
      "Epoch 24, CIFAR-10 Batch 4:  Accuracy: 0.4228000044822693 Loss: 1.5364234447479248\n",
      "Epoch 24, CIFAR-10 Batch 5:  Accuracy: 0.41819998621940613 Loss: 1.5261642932891846\n",
      "Epoch 25, CIFAR-10 Batch 1:  Accuracy: 0.4251999855041504 Loss: 1.6042133569717407\n",
      "Epoch 25, CIFAR-10 Batch 2:  Accuracy: 0.4307999908924103 Loss: 1.648711919784546\n",
      "Epoch 25, CIFAR-10 Batch 3:  Accuracy: 0.43299999833106995 Loss: 1.4024875164031982\n",
      "Epoch 25, CIFAR-10 Batch 4:  Accuracy: 0.4284000098705292 Loss: 1.502768874168396\n",
      "Epoch 25, CIFAR-10 Batch 5:  Accuracy: 0.43140000104904175 Loss: 1.535227656364441\n",
      "Epoch 26, CIFAR-10 Batch 1:  Accuracy: 0.43140000104904175 Loss: 1.5786499977111816\n",
      "Epoch 26, CIFAR-10 Batch 2:  Accuracy: 0.43299999833106995 Loss: 1.6579959392547607\n",
      "Epoch 26, CIFAR-10 Batch 3:  Accuracy: 0.43459999561309814 Loss: 1.3966814279556274\n",
      "Epoch 26, CIFAR-10 Batch 4:  Accuracy: 0.4327999949455261 Loss: 1.4579527378082275\n",
      "Epoch 26, CIFAR-10 Batch 5:  Accuracy: 0.4327999949455261 Loss: 1.5302050113677979\n",
      "Epoch 27, CIFAR-10 Batch 1:  Accuracy: 0.4352000057697296 Loss: 1.5914596319198608\n",
      "Epoch 27, CIFAR-10 Batch 2:  Accuracy: 0.4325999915599823 Loss: 1.6221866607666016\n",
      "Epoch 27, CIFAR-10 Batch 3:  Accuracy: 0.44020000100135803 Loss: 1.3716015815734863\n",
      "Epoch 27, CIFAR-10 Batch 4:  Accuracy: 0.43799999356269836 Loss: 1.4281346797943115\n",
      "Epoch 27, CIFAR-10 Batch 5:  Accuracy: 0.43479999899864197 Loss: 1.533652901649475\n",
      "Epoch 28, CIFAR-10 Batch 1:  Accuracy: 0.4413999915122986 Loss: 1.5323880910873413\n",
      "Epoch 28, CIFAR-10 Batch 2:  Accuracy: 0.4381999969482422 Loss: 1.6007188558578491\n",
      "Epoch 28, CIFAR-10 Batch 3:  Accuracy: 0.4438000023365021 Loss: 1.3640614748001099\n",
      "Epoch 28, CIFAR-10 Batch 4:  Accuracy: 0.4413999915122986 Loss: 1.3819133043289185\n",
      "Epoch 28, CIFAR-10 Batch 5:  Accuracy: 0.43779999017715454 Loss: 1.472799301147461\n",
      "Epoch 29, CIFAR-10 Batch 1:  Accuracy: 0.4415999948978424 Loss: 1.5083353519439697\n",
      "Epoch 29, CIFAR-10 Batch 2:  Accuracy: 0.4415999948978424 Loss: 1.5763874053955078\n",
      "Epoch 29, CIFAR-10 Batch 3:  Accuracy: 0.4474000036716461 Loss: 1.3571964502334595\n",
      "Epoch 29, CIFAR-10 Batch 4:  Accuracy: 0.4514000117778778 Loss: 1.3486251831054688\n",
      "Epoch 29, CIFAR-10 Batch 5:  Accuracy: 0.4413999915122986 Loss: 1.480743646621704\n",
      "Epoch 30, CIFAR-10 Batch 1:  Accuracy: 0.44600000977516174 Loss: 1.5149180889129639\n",
      "Epoch 30, CIFAR-10 Batch 2:  Accuracy: 0.44200000166893005 Loss: 1.5464736223220825\n",
      "Epoch 30, CIFAR-10 Batch 3:  Accuracy: 0.4480000138282776 Loss: 1.3428953886032104\n",
      "Epoch 30, CIFAR-10 Batch 4:  Accuracy: 0.4431999921798706 Loss: 1.3616256713867188\n",
      "Epoch 30, CIFAR-10 Batch 5:  Accuracy: 0.44600000977516174 Loss: 1.4856042861938477\n",
      "Epoch 31, CIFAR-10 Batch 1:  Accuracy: 0.4440000057220459 Loss: 1.5151569843292236\n",
      "Epoch 31, CIFAR-10 Batch 2:  Accuracy: 0.44440001249313354 Loss: 1.5359861850738525\n",
      "Epoch 31, CIFAR-10 Batch 3:  Accuracy: 0.446399986743927 Loss: 1.3249714374542236\n",
      "Epoch 31, CIFAR-10 Batch 4:  Accuracy: 0.44780001044273376 Loss: 1.3080427646636963\n",
      "Epoch 31, CIFAR-10 Batch 5:  Accuracy: 0.4480000138282776 Loss: 1.4686689376831055\n",
      "Epoch 32, CIFAR-10 Batch 1:  Accuracy: 0.4465999901294708 Loss: 1.476007342338562\n",
      "Epoch 32, CIFAR-10 Batch 2:  Accuracy: 0.4453999996185303 Loss: 1.548372507095337\n",
      "Epoch 32, CIFAR-10 Batch 3:  Accuracy: 0.44839999079704285 Loss: 1.3267723321914673\n",
      "Epoch 32, CIFAR-10 Batch 4:  Accuracy: 0.4442000091075897 Loss: 1.2778635025024414\n",
      "Epoch 32, CIFAR-10 Batch 5:  Accuracy: 0.4474000036716461 Loss: 1.4562164545059204\n",
      "Epoch 33, CIFAR-10 Batch 1:  Accuracy: 0.4496000111103058 Loss: 1.47283136844635\n",
      "Epoch 33, CIFAR-10 Batch 2:  Accuracy: 0.446399986743927 Loss: 1.503272533416748\n",
      "Epoch 33, CIFAR-10 Batch 3:  Accuracy: 0.45100000500679016 Loss: 1.319262981414795\n",
      "Epoch 33, CIFAR-10 Batch 4:  Accuracy: 0.45179998874664307 Loss: 1.2920475006103516\n",
      "Epoch 33, CIFAR-10 Batch 5:  Accuracy: 0.44780001044273376 Loss: 1.4369895458221436\n",
      "Epoch 34, CIFAR-10 Batch 1:  Accuracy: 0.45239999890327454 Loss: 1.4419314861297607\n",
      "Epoch 34, CIFAR-10 Batch 2:  Accuracy: 0.44760000705718994 Loss: 1.4965568780899048\n",
      "Epoch 34, CIFAR-10 Batch 3:  Accuracy: 0.44920000433921814 Loss: 1.3130956888198853\n",
      "Epoch 34, CIFAR-10 Batch 4:  Accuracy: 0.4521999955177307 Loss: 1.2739126682281494\n",
      "Epoch 34, CIFAR-10 Batch 5:  Accuracy: 0.4519999921321869 Loss: 1.3918653726577759\n",
      "Epoch 35, CIFAR-10 Batch 1:  Accuracy: 0.45080000162124634 Loss: 1.45854914188385\n",
      "Epoch 35, CIFAR-10 Batch 2:  Accuracy: 0.4521999955177307 Loss: 1.471285343170166\n",
      "Epoch 35, CIFAR-10 Batch 3:  Accuracy: 0.4546000063419342 Loss: 1.2963281869888306\n",
      "Epoch 35, CIFAR-10 Batch 4:  Accuracy: 0.4578000009059906 Loss: 1.2613011598587036\n",
      "Epoch 35, CIFAR-10 Batch 5:  Accuracy: 0.4521999955177307 Loss: 1.4199235439300537\n",
      "Epoch 36, CIFAR-10 Batch 1:  Accuracy: 0.4569999873638153 Loss: 1.4105632305145264\n",
      "Epoch 36, CIFAR-10 Batch 2:  Accuracy: 0.44999998807907104 Loss: 1.4393329620361328\n",
      "Epoch 36, CIFAR-10 Batch 3:  Accuracy: 0.4560000002384186 Loss: 1.2719873189926147\n",
      "Epoch 36, CIFAR-10 Batch 4:  Accuracy: 0.4607999920845032 Loss: 1.2500158548355103\n",
      "Epoch 36, CIFAR-10 Batch 5:  Accuracy: 0.45559999346733093 Loss: 1.3828973770141602\n",
      "Epoch 37, CIFAR-10 Batch 1:  Accuracy: 0.46000000834465027 Loss: 1.410717248916626\n",
      "Epoch 37, CIFAR-10 Batch 2:  Accuracy: 0.4551999866962433 Loss: 1.4222952127456665\n",
      "Epoch 37, CIFAR-10 Batch 3:  Accuracy: 0.4553999900817871 Loss: 1.2749052047729492\n",
      "Epoch 37, CIFAR-10 Batch 4:  Accuracy: 0.4611999988555908 Loss: 1.2402607202529907\n",
      "Epoch 37, CIFAR-10 Batch 5:  Accuracy: 0.45840001106262207 Loss: 1.3911609649658203\n",
      "Epoch 38, CIFAR-10 Batch 1:  Accuracy: 0.4620000123977661 Loss: 1.3719167709350586\n",
      "Epoch 38, CIFAR-10 Batch 2:  Accuracy: 0.45339998602867126 Loss: 1.414501667022705\n",
      "Epoch 38, CIFAR-10 Batch 3:  Accuracy: 0.45260000228881836 Loss: 1.2684824466705322\n",
      "Epoch 38, CIFAR-10 Batch 4:  Accuracy: 0.45980000495910645 Loss: 1.2561661005020142\n",
      "Epoch 38, CIFAR-10 Batch 5:  Accuracy: 0.4618000090122223 Loss: 1.388092279434204\n",
      "Epoch 39, CIFAR-10 Batch 1:  Accuracy: 0.46560001373291016 Loss: 1.3773179054260254\n",
      "Epoch 39, CIFAR-10 Batch 2:  Accuracy: 0.4569999873638153 Loss: 1.3663883209228516\n",
      "Epoch 39, CIFAR-10 Batch 3:  Accuracy: 0.46140000224113464 Loss: 1.2680742740631104\n",
      "Epoch 39, CIFAR-10 Batch 4:  Accuracy: 0.4702000021934509 Loss: 1.2231271266937256\n",
      "Epoch 39, CIFAR-10 Batch 5:  Accuracy: 0.460999995470047 Loss: 1.4007333517074585\n",
      "Epoch 40, CIFAR-10 Batch 1:  Accuracy: 0.46399998664855957 Loss: 1.3627912998199463\n",
      "Epoch 40, CIFAR-10 Batch 2:  Accuracy: 0.4596000015735626 Loss: 1.3751533031463623\n",
      "Epoch 40, CIFAR-10 Batch 3:  Accuracy: 0.4593999981880188 Loss: 1.231994867324829\n",
      "Epoch 40, CIFAR-10 Batch 4:  Accuracy: 0.46380001306533813 Loss: 1.227757215499878\n",
      "Epoch 40, CIFAR-10 Batch 5:  Accuracy: 0.4674000144004822 Loss: 1.3902480602264404\n",
      "Epoch 41, CIFAR-10 Batch 1:  Accuracy: 0.4717999994754791 Loss: 1.3470121622085571\n",
      "Epoch 41, CIFAR-10 Batch 2:  Accuracy: 0.4652000069618225 Loss: 1.366864562034607\n",
      "Epoch 41, CIFAR-10 Batch 3:  Accuracy: 0.4607999920845032 Loss: 1.247252345085144\n",
      "Epoch 41, CIFAR-10 Batch 4:  Accuracy: 0.46880000829696655 Loss: 1.2195014953613281\n",
      "Epoch 41, CIFAR-10 Batch 5:  Accuracy: 0.46380001306533813 Loss: 1.3856791257858276\n",
      "Epoch 42, CIFAR-10 Batch 1:  Accuracy: 0.4706000089645386 Loss: 1.3626887798309326\n",
      "Epoch 42, CIFAR-10 Batch 2:  Accuracy: 0.4611999988555908 Loss: 1.3685696125030518\n",
      "Epoch 42, CIFAR-10 Batch 3:  Accuracy: 0.4636000096797943 Loss: 1.246137261390686\n",
      "Epoch 42, CIFAR-10 Batch 4:  Accuracy: 0.46560001373291016 Loss: 1.1933588981628418\n",
      "Epoch 42, CIFAR-10 Batch 5:  Accuracy: 0.4684000015258789 Loss: 1.3762996196746826\n",
      "Epoch 43, CIFAR-10 Batch 1:  Accuracy: 0.4733999967575073 Loss: 1.3258259296417236\n",
      "Epoch 43, CIFAR-10 Batch 2:  Accuracy: 0.4643999934196472 Loss: 1.346939206123352\n",
      "Epoch 43, CIFAR-10 Batch 3:  Accuracy: 0.46639999747276306 Loss: 1.226625680923462\n",
      "Epoch 43, CIFAR-10 Batch 4:  Accuracy: 0.4681999981403351 Loss: 1.2036411762237549\n",
      "Epoch 43, CIFAR-10 Batch 5:  Accuracy: 0.47119998931884766 Loss: 1.3862497806549072\n",
      "Epoch 44, CIFAR-10 Batch 1:  Accuracy: 0.46880000829696655 Loss: 1.325295329093933\n",
      "Epoch 44, CIFAR-10 Batch 2:  Accuracy: 0.46459999680519104 Loss: 1.3158245086669922\n",
      "Epoch 44, CIFAR-10 Batch 3:  Accuracy: 0.46880000829696655 Loss: 1.2124395370483398\n",
      "Epoch 44, CIFAR-10 Batch 4:  Accuracy: 0.4745999872684479 Loss: 1.1476486921310425\n",
      "Epoch 44, CIFAR-10 Batch 5:  Accuracy: 0.4713999927043915 Loss: 1.3871098756790161\n",
      "Epoch 45, CIFAR-10 Batch 1:  Accuracy: 0.4779999852180481 Loss: 1.3230525255203247\n",
      "Epoch 45, CIFAR-10 Batch 2:  Accuracy: 0.46959999203681946 Loss: 1.2745001316070557\n",
      "Epoch 45, CIFAR-10 Batch 3:  Accuracy: 0.46939998865127563 Loss: 1.1872060298919678\n",
      "Epoch 45, CIFAR-10 Batch 4:  Accuracy: 0.4729999899864197 Loss: 1.1726961135864258\n",
      "Epoch 45, CIFAR-10 Batch 5:  Accuracy: 0.47360000014305115 Loss: 1.3591684103012085\n",
      "Epoch 46, CIFAR-10 Batch 1:  Accuracy: 0.4708000123500824 Loss: 1.3114533424377441\n",
      "Epoch 46, CIFAR-10 Batch 2:  Accuracy: 0.4674000144004822 Loss: 1.2849241495132446\n",
      "Epoch 46, CIFAR-10 Batch 3:  Accuracy: 0.4668000042438507 Loss: 1.2023746967315674\n",
      "Epoch 46, CIFAR-10 Batch 4:  Accuracy: 0.4729999899864197 Loss: 1.152035117149353\n",
      "Epoch 46, CIFAR-10 Batch 5:  Accuracy: 0.4758000075817108 Loss: 1.342883825302124\n",
      "Epoch 47, CIFAR-10 Batch 1:  Accuracy: 0.47839999198913574 Loss: 1.2949228286743164\n",
      "Epoch 47, CIFAR-10 Batch 2:  Accuracy: 0.4702000021934509 Loss: 1.2759593725204468\n",
      "Epoch 47, CIFAR-10 Batch 3:  Accuracy: 0.47279998660087585 Loss: 1.217487096786499\n",
      "Epoch 47, CIFAR-10 Batch 4:  Accuracy: 0.4797999858856201 Loss: 1.151943564414978\n",
      "Epoch 47, CIFAR-10 Batch 5:  Accuracy: 0.47999998927116394 Loss: 1.3435510396957397\n",
      "Epoch 48, CIFAR-10 Batch 1:  Accuracy: 0.47839999198913574 Loss: 1.2835047245025635\n",
      "Epoch 48, CIFAR-10 Batch 2:  Accuracy: 0.4731999933719635 Loss: 1.2606890201568604\n",
      "Epoch 48, CIFAR-10 Batch 3:  Accuracy: 0.47200000286102295 Loss: 1.1738336086273193\n",
      "Epoch 48, CIFAR-10 Batch 4:  Accuracy: 0.47920000553131104 Loss: 1.138556718826294\n",
      "Epoch 48, CIFAR-10 Batch 5:  Accuracy: 0.48339998722076416 Loss: 1.3525657653808594\n",
      "Epoch 49, CIFAR-10 Batch 1:  Accuracy: 0.475600004196167 Loss: 1.2762622833251953\n",
      "Epoch 49, CIFAR-10 Batch 2:  Accuracy: 0.47780001163482666 Loss: 1.3056479692459106\n",
      "Epoch 49, CIFAR-10 Batch 3:  Accuracy: 0.47360000014305115 Loss: 1.170091986656189\n",
      "Epoch 49, CIFAR-10 Batch 4:  Accuracy: 0.47780001163482666 Loss: 1.1355340480804443\n",
      "Epoch 49, CIFAR-10 Batch 5:  Accuracy: 0.47920000553131104 Loss: 1.32206130027771\n",
      "Epoch 50, CIFAR-10 Batch 1:  Accuracy: 0.47920000553131104 Loss: 1.3247120380401611\n",
      "Epoch 50, CIFAR-10 Batch 2:  Accuracy: 0.4796000123023987 Loss: 1.285657525062561\n",
      "Epoch 50, CIFAR-10 Batch 3:  Accuracy: 0.47279998660087585 Loss: 1.1627241373062134\n",
      "Epoch 50, CIFAR-10 Batch 4:  Accuracy: 0.4814000129699707 Loss: 1.1588083505630493\n",
      "Epoch 50, CIFAR-10 Batch 5:  Accuracy: 0.4864000082015991 Loss: 1.30521559715271\n",
      "Epoch 51, CIFAR-10 Batch 1:  Accuracy: 0.4781999886035919 Loss: 1.271268606185913\n",
      "Epoch 51, CIFAR-10 Batch 2:  Accuracy: 0.47620001435279846 Loss: 1.2861809730529785\n",
      "Epoch 51, CIFAR-10 Batch 3:  Accuracy: 0.47839999198913574 Loss: 1.1335748434066772\n",
      "Epoch 51, CIFAR-10 Batch 4:  Accuracy: 0.4860000014305115 Loss: 1.120502233505249\n",
      "Epoch 51, CIFAR-10 Batch 5:  Accuracy: 0.4885999858379364 Loss: 1.2835948467254639\n",
      "Epoch 52, CIFAR-10 Batch 1:  Accuracy: 0.4819999933242798 Loss: 1.2644771337509155\n",
      "Epoch 52, CIFAR-10 Batch 2:  Accuracy: 0.4819999933242798 Loss: 1.2859556674957275\n",
      "Epoch 52, CIFAR-10 Batch 3:  Accuracy: 0.47920000553131104 Loss: 1.1281538009643555\n",
      "Epoch 52, CIFAR-10 Batch 4:  Accuracy: 0.48559999465942383 Loss: 1.1189796924591064\n",
      "Epoch 52, CIFAR-10 Batch 5:  Accuracy: 0.4878000020980835 Loss: 1.277106761932373\n",
      "Epoch 53, CIFAR-10 Batch 1:  Accuracy: 0.48420000076293945 Loss: 1.2832441329956055\n",
      "Epoch 53, CIFAR-10 Batch 2:  Accuracy: 0.4851999878883362 Loss: 1.2971410751342773\n",
      "Epoch 53, CIFAR-10 Batch 3:  Accuracy: 0.4828000068664551 Loss: 1.106341004371643\n",
      "Epoch 53, CIFAR-10 Batch 4:  Accuracy: 0.4896000027656555 Loss: 1.1184242963790894\n",
      "Epoch 53, CIFAR-10 Batch 5:  Accuracy: 0.49000000953674316 Loss: 1.3164379596710205\n",
      "Epoch 54, CIFAR-10 Batch 1:  Accuracy: 0.4805999994277954 Loss: 1.2445900440216064\n",
      "Epoch 54, CIFAR-10 Batch 2:  Accuracy: 0.4797999858856201 Loss: 1.288030982017517\n",
      "Epoch 54, CIFAR-10 Batch 3:  Accuracy: 0.4803999960422516 Loss: 1.1235829591751099\n",
      "Epoch 54, CIFAR-10 Batch 4:  Accuracy: 0.48240000009536743 Loss: 1.118520736694336\n",
      "Epoch 54, CIFAR-10 Batch 5:  Accuracy: 0.48539999127388 Loss: 1.3165557384490967\n",
      "Epoch 55, CIFAR-10 Batch 1:  Accuracy: 0.48420000076293945 Loss: 1.2656477689743042\n",
      "Epoch 55, CIFAR-10 Batch 2:  Accuracy: 0.4805999994277954 Loss: 1.2746994495391846\n",
      "Epoch 55, CIFAR-10 Batch 3:  Accuracy: 0.48339998722076416 Loss: 1.108560562133789\n",
      "Epoch 55, CIFAR-10 Batch 4:  Accuracy: 0.4950000047683716 Loss: 1.1078964471817017\n",
      "Epoch 55, CIFAR-10 Batch 5:  Accuracy: 0.4869999885559082 Loss: 1.28287672996521\n",
      "Epoch 56, CIFAR-10 Batch 1:  Accuracy: 0.4896000027656555 Loss: 1.280674695968628\n",
      "Epoch 56, CIFAR-10 Batch 2:  Accuracy: 0.4812000095844269 Loss: 1.3163009881973267\n",
      "Epoch 56, CIFAR-10 Batch 3:  Accuracy: 0.48420000076293945 Loss: 1.0707976818084717\n",
      "Epoch 56, CIFAR-10 Batch 4:  Accuracy: 0.4896000027656555 Loss: 1.0893235206604004\n",
      "Epoch 56, CIFAR-10 Batch 5:  Accuracy: 0.4864000082015991 Loss: 1.286577820777893\n",
      "Epoch 57, CIFAR-10 Batch 1:  Accuracy: 0.48899999260902405 Loss: 1.2408884763717651\n",
      "Epoch 57, CIFAR-10 Batch 2:  Accuracy: 0.4862000048160553 Loss: 1.269717812538147\n",
      "Epoch 57, CIFAR-10 Batch 3:  Accuracy: 0.4896000027656555 Loss: 1.0691421031951904\n",
      "Epoch 57, CIFAR-10 Batch 4:  Accuracy: 0.49300000071525574 Loss: 1.116808295249939\n",
      "Epoch 57, CIFAR-10 Batch 5:  Accuracy: 0.48840001225471497 Loss: 1.2638400793075562\n",
      "Epoch 58, CIFAR-10 Batch 1:  Accuracy: 0.4860000014305115 Loss: 1.2356573343276978\n",
      "Epoch 58, CIFAR-10 Batch 2:  Accuracy: 0.4819999933242798 Loss: 1.2668629884719849\n",
      "Epoch 58, CIFAR-10 Batch 3:  Accuracy: 0.47940000891685486 Loss: 1.0501301288604736\n",
      "Epoch 58, CIFAR-10 Batch 4:  Accuracy: 0.4986000061035156 Loss: 1.0892671346664429\n",
      "Epoch 58, CIFAR-10 Batch 5:  Accuracy: 0.49380001425743103 Loss: 1.229151725769043\n",
      "Epoch 59, CIFAR-10 Batch 1:  Accuracy: 0.48559999465942383 Loss: 1.2224067449569702\n",
      "Epoch 59, CIFAR-10 Batch 2:  Accuracy: 0.4832000136375427 Loss: 1.2581164836883545\n",
      "Epoch 59, CIFAR-10 Batch 3:  Accuracy: 0.48579999804496765 Loss: 1.0509812831878662\n",
      "Epoch 59, CIFAR-10 Batch 4:  Accuracy: 0.48919999599456787 Loss: 1.0713732242584229\n",
      "Epoch 59, CIFAR-10 Batch 5:  Accuracy: 0.4934000074863434 Loss: 1.2695374488830566\n",
      "Epoch 60, CIFAR-10 Batch 1:  Accuracy: 0.487199991941452 Loss: 1.2397749423980713\n",
      "Epoch 60, CIFAR-10 Batch 2:  Accuracy: 0.4837999939918518 Loss: 1.2870409488677979\n",
      "Epoch 60, CIFAR-10 Batch 3:  Accuracy: 0.4851999878883362 Loss: 1.0327180624008179\n",
      "Epoch 60, CIFAR-10 Batch 4:  Accuracy: 0.4991999864578247 Loss: 1.0764274597167969\n",
      "Epoch 60, CIFAR-10 Batch 5:  Accuracy: 0.4973999857902527 Loss: 1.2218719720840454\n",
      "Epoch 61, CIFAR-10 Batch 1:  Accuracy: 0.49059998989105225 Loss: 1.2445873022079468\n",
      "Epoch 61, CIFAR-10 Batch 2:  Accuracy: 0.48919999599456787 Loss: 1.2429770231246948\n",
      "Epoch 61, CIFAR-10 Batch 3:  Accuracy: 0.48660001158714294 Loss: 1.0547499656677246\n",
      "Epoch 61, CIFAR-10 Batch 4:  Accuracy: 0.49939998984336853 Loss: 1.06300950050354\n",
      "Epoch 61, CIFAR-10 Batch 5:  Accuracy: 0.49559998512268066 Loss: 1.2407845258712769\n",
      "Epoch 62, CIFAR-10 Batch 1:  Accuracy: 0.48899999260902405 Loss: 1.2454192638397217\n",
      "Epoch 62, CIFAR-10 Batch 2:  Accuracy: 0.4844000041484833 Loss: 1.241653561592102\n",
      "Epoch 62, CIFAR-10 Batch 3:  Accuracy: 0.4812000095844269 Loss: 1.0337040424346924\n",
      "Epoch 62, CIFAR-10 Batch 4:  Accuracy: 0.49380001425743103 Loss: 1.0799468755722046\n",
      "Epoch 62, CIFAR-10 Batch 5:  Accuracy: 0.4957999885082245 Loss: 1.2317395210266113\n",
      "Epoch 63, CIFAR-10 Batch 1:  Accuracy: 0.4893999993801117 Loss: 1.208815097808838\n",
      "Epoch 63, CIFAR-10 Batch 2:  Accuracy: 0.4912000000476837 Loss: 1.2551835775375366\n",
      "Epoch 63, CIFAR-10 Batch 3:  Accuracy: 0.4819999933242798 Loss: 1.0445636510849\n",
      "Epoch 63, CIFAR-10 Batch 4:  Accuracy: 0.49880000948905945 Loss: 1.0607140064239502\n",
      "Epoch 63, CIFAR-10 Batch 5:  Accuracy: 0.49559998512268066 Loss: 1.2843496799468994\n",
      "Epoch 64, CIFAR-10 Batch 1:  Accuracy: 0.487199991941452 Loss: 1.2688090801239014\n",
      "Epoch 64, CIFAR-10 Batch 2:  Accuracy: 0.4903999865055084 Loss: 1.2482578754425049\n",
      "Epoch 64, CIFAR-10 Batch 3:  Accuracy: 0.48980000615119934 Loss: 1.030706763267517\n",
      "Epoch 64, CIFAR-10 Batch 4:  Accuracy: 0.5012000203132629 Loss: 1.06571364402771\n",
      "Epoch 64, CIFAR-10 Batch 5:  Accuracy: 0.49900001287460327 Loss: 1.2257096767425537\n",
      "Epoch 65, CIFAR-10 Batch 1:  Accuracy: 0.49380001425743103 Loss: 1.1925221681594849\n",
      "Epoch 65, CIFAR-10 Batch 2:  Accuracy: 0.49079999327659607 Loss: 1.2284724712371826\n",
      "Epoch 65, CIFAR-10 Batch 3:  Accuracy: 0.490200012922287 Loss: 1.0110952854156494\n",
      "Epoch 65, CIFAR-10 Batch 4:  Accuracy: 0.4950000047683716 Loss: 1.0729589462280273\n",
      "Epoch 65, CIFAR-10 Batch 5:  Accuracy: 0.5008000135421753 Loss: 1.2305586338043213\n",
      "Epoch 66, CIFAR-10 Batch 1:  Accuracy: 0.4934000074863434 Loss: 1.175158977508545\n",
      "Epoch 66, CIFAR-10 Batch 2:  Accuracy: 0.4896000027656555 Loss: 1.2103259563446045\n",
      "Epoch 66, CIFAR-10 Batch 3:  Accuracy: 0.48919999599456787 Loss: 1.0328296422958374\n",
      "Epoch 66, CIFAR-10 Batch 4:  Accuracy: 0.5059999823570251 Loss: 1.0558992624282837\n",
      "Epoch 66, CIFAR-10 Batch 5:  Accuracy: 0.4973999857902527 Loss: 1.2414902448654175\n",
      "Epoch 67, CIFAR-10 Batch 1:  Accuracy: 0.4984000027179718 Loss: 1.1761665344238281\n",
      "Epoch 67, CIFAR-10 Batch 2:  Accuracy: 0.49079999327659607 Loss: 1.1962846517562866\n",
      "Epoch 67, CIFAR-10 Batch 3:  Accuracy: 0.48980000615119934 Loss: 1.0179402828216553\n",
      "Epoch 67, CIFAR-10 Batch 4:  Accuracy: 0.4991999864578247 Loss: 1.0554050207138062\n",
      "Epoch 67, CIFAR-10 Batch 5:  Accuracy: 0.49380001425743103 Loss: 1.2286790609359741\n",
      "Epoch 68, CIFAR-10 Batch 1:  Accuracy: 0.49380001425743103 Loss: 1.1722700595855713\n",
      "Epoch 68, CIFAR-10 Batch 2:  Accuracy: 0.4846000075340271 Loss: 1.1706651449203491\n",
      "Epoch 68, CIFAR-10 Batch 3:  Accuracy: 0.48539999127388 Loss: 1.0128275156021118\n",
      "Epoch 68, CIFAR-10 Batch 4:  Accuracy: 0.4986000061035156 Loss: 1.0694175958633423\n",
      "Epoch 68, CIFAR-10 Batch 5:  Accuracy: 0.4970000088214874 Loss: 1.2089173793792725\n",
      "Epoch 69, CIFAR-10 Batch 1:  Accuracy: 0.49900001287460327 Loss: 1.1816798448562622\n",
      "Epoch 69, CIFAR-10 Batch 2:  Accuracy: 0.4925999939441681 Loss: 1.1929563283920288\n",
      "Epoch 69, CIFAR-10 Batch 3:  Accuracy: 0.48399999737739563 Loss: 1.013653039932251\n",
      "Epoch 69, CIFAR-10 Batch 4:  Accuracy: 0.49959999322891235 Loss: 1.0359337329864502\n",
      "Epoch 69, CIFAR-10 Batch 5:  Accuracy: 0.498199999332428 Loss: 1.2147512435913086\n",
      "Epoch 70, CIFAR-10 Batch 1:  Accuracy: 0.492000013589859 Loss: 1.178331732749939\n",
      "Epoch 70, CIFAR-10 Batch 2:  Accuracy: 0.4837999939918518 Loss: 1.1749986410140991\n",
      "Epoch 70, CIFAR-10 Batch 3:  Accuracy: 0.4896000027656555 Loss: 1.0053212642669678\n",
      "Epoch 70, CIFAR-10 Batch 4:  Accuracy: 0.5016000270843506 Loss: 1.0183985233306885\n",
      "Epoch 70, CIFAR-10 Batch 5:  Accuracy: 0.49720001220703125 Loss: 1.23142671585083\n",
      "Epoch 71, CIFAR-10 Batch 1:  Accuracy: 0.49480000138282776 Loss: 1.2080436944961548\n",
      "Epoch 71, CIFAR-10 Batch 2:  Accuracy: 0.48660001158714294 Loss: 1.1816141605377197\n",
      "Epoch 71, CIFAR-10 Batch 3:  Accuracy: 0.4844000041484833 Loss: 1.025994896888733\n",
      "Epoch 71, CIFAR-10 Batch 4:  Accuracy: 0.503000020980835 Loss: 1.045357346534729\n",
      "Epoch 71, CIFAR-10 Batch 5:  Accuracy: 0.5012000203132629 Loss: 1.2122502326965332\n",
      "Epoch 72, CIFAR-10 Batch 1:  Accuracy: 0.4950000047683716 Loss: 1.1768600940704346\n",
      "Epoch 72, CIFAR-10 Batch 2:  Accuracy: 0.4885999858379364 Loss: 1.1869639158248901\n",
      "Epoch 72, CIFAR-10 Batch 3:  Accuracy: 0.48840001225471497 Loss: 0.9981096386909485\n",
      "Epoch 72, CIFAR-10 Batch 4:  Accuracy: 0.5019999742507935 Loss: 1.002912998199463\n",
      "Epoch 72, CIFAR-10 Batch 5:  Accuracy: 0.49880000948905945 Loss: 1.239467978477478\n",
      "Epoch 73, CIFAR-10 Batch 1:  Accuracy: 0.4941999912261963 Loss: 1.1785528659820557\n",
      "Epoch 73, CIFAR-10 Batch 2:  Accuracy: 0.49619999527931213 Loss: 1.174391746520996\n",
      "Epoch 73, CIFAR-10 Batch 3:  Accuracy: 0.4941999912261963 Loss: 1.0490230321884155\n",
      "Epoch 73, CIFAR-10 Batch 4:  Accuracy: 0.5108000040054321 Loss: 0.9883371591567993\n",
      "Epoch 73, CIFAR-10 Batch 5:  Accuracy: 0.5049999952316284 Loss: 1.2144584655761719\n",
      "Epoch 74, CIFAR-10 Batch 1:  Accuracy: 0.49799999594688416 Loss: 1.1877074241638184\n",
      "Epoch 74, CIFAR-10 Batch 2:  Accuracy: 0.48539999127388 Loss: 1.1724843978881836\n",
      "Epoch 74, CIFAR-10 Batch 3:  Accuracy: 0.49459999799728394 Loss: 1.0068943500518799\n",
      "Epoch 74, CIFAR-10 Batch 4:  Accuracy: 0.5026000142097473 Loss: 1.0223910808563232\n",
      "Epoch 74, CIFAR-10 Batch 5:  Accuracy: 0.5085999965667725 Loss: 1.1624338626861572\n",
      "Epoch 75, CIFAR-10 Batch 1:  Accuracy: 0.4966000020503998 Loss: 1.1832976341247559\n",
      "Epoch 75, CIFAR-10 Batch 2:  Accuracy: 0.49000000953674316 Loss: 1.1572544574737549\n",
      "Epoch 75, CIFAR-10 Batch 3:  Accuracy: 0.49239999055862427 Loss: 0.9979369044303894\n",
      "Epoch 75, CIFAR-10 Batch 4:  Accuracy: 0.5052000284194946 Loss: 1.0263131856918335\n",
      "Epoch 75, CIFAR-10 Batch 5:  Accuracy: 0.5040000081062317 Loss: 1.1877018213272095\n",
      "Epoch 76, CIFAR-10 Batch 1:  Accuracy: 0.49380001425743103 Loss: 1.1912120580673218\n",
      "Epoch 76, CIFAR-10 Batch 2:  Accuracy: 0.4925999939441681 Loss: 1.1415972709655762\n",
      "Epoch 76, CIFAR-10 Batch 3:  Accuracy: 0.4887999892234802 Loss: 1.0095659494400024\n",
      "Epoch 76, CIFAR-10 Batch 4:  Accuracy: 0.5005999803543091 Loss: 1.0086787939071655\n",
      "Epoch 76, CIFAR-10 Batch 5:  Accuracy: 0.4984000027179718 Loss: 1.197766661643982\n",
      "Epoch 77, CIFAR-10 Batch 1:  Accuracy: 0.4950000047683716 Loss: 1.128539800643921\n",
      "Epoch 77, CIFAR-10 Batch 2:  Accuracy: 0.4896000027656555 Loss: 1.167711853981018\n",
      "Epoch 77, CIFAR-10 Batch 3:  Accuracy: 0.4896000027656555 Loss: 1.0126454830169678\n",
      "Epoch 77, CIFAR-10 Batch 4:  Accuracy: 0.5054000020027161 Loss: 0.9959473609924316\n",
      "Epoch 77, CIFAR-10 Batch 5:  Accuracy: 0.5031999945640564 Loss: 1.1892040967941284\n",
      "Epoch 78, CIFAR-10 Batch 1:  Accuracy: 0.4975999891757965 Loss: 1.169641375541687\n",
      "Epoch 78, CIFAR-10 Batch 2:  Accuracy: 0.49779999256134033 Loss: 1.1183327436447144\n",
      "Epoch 78, CIFAR-10 Batch 3:  Accuracy: 0.4952000081539154 Loss: 0.9949140548706055\n",
      "Epoch 78, CIFAR-10 Batch 4:  Accuracy: 0.5052000284194946 Loss: 1.025874137878418\n",
      "Epoch 78, CIFAR-10 Batch 5:  Accuracy: 0.5037999749183655 Loss: 1.2203267812728882\n",
      "Epoch 79, CIFAR-10 Batch 1:  Accuracy: 0.5019999742507935 Loss: 1.1903892755508423\n",
      "Epoch 79, CIFAR-10 Batch 2:  Accuracy: 0.4941999912261963 Loss: 1.144984483718872\n",
      "Epoch 79, CIFAR-10 Batch 3:  Accuracy: 0.4909999966621399 Loss: 1.010839581489563\n",
      "Epoch 79, CIFAR-10 Batch 4:  Accuracy: 0.49959999322891235 Loss: 1.0206470489501953\n",
      "Epoch 79, CIFAR-10 Batch 5:  Accuracy: 0.5034000277519226 Loss: 1.2264481782913208\n",
      "Epoch 80, CIFAR-10 Batch 1:  Accuracy: 0.5001999735832214 Loss: 1.1472899913787842\n",
      "Epoch 80, CIFAR-10 Batch 2:  Accuracy: 0.49779999256134033 Loss: 1.1132080554962158\n",
      "Epoch 80, CIFAR-10 Batch 3:  Accuracy: 0.4903999865055084 Loss: 0.9594367742538452\n",
      "Epoch 80, CIFAR-10 Batch 4:  Accuracy: 0.5054000020027161 Loss: 1.0042625665664673\n",
      "Epoch 80, CIFAR-10 Batch 5:  Accuracy: 0.5085999965667725 Loss: 1.1927947998046875\n",
      "Epoch 81, CIFAR-10 Batch 1:  Accuracy: 0.5005999803543091 Loss: 1.1412593126296997\n",
      "Epoch 81, CIFAR-10 Batch 2:  Accuracy: 0.4950000047683716 Loss: 1.1083557605743408\n",
      "Epoch 81, CIFAR-10 Batch 3:  Accuracy: 0.49300000071525574 Loss: 0.9652227163314819\n",
      "Epoch 81, CIFAR-10 Batch 4:  Accuracy: 0.5040000081062317 Loss: 1.007463812828064\n",
      "Epoch 81, CIFAR-10 Batch 5:  Accuracy: 0.5023999810218811 Loss: 1.2256276607513428\n",
      "Epoch 82, CIFAR-10 Batch 1:  Accuracy: 0.5013999938964844 Loss: 1.1245416402816772\n",
      "Epoch 82, CIFAR-10 Batch 2:  Accuracy: 0.49779999256134033 Loss: 1.107367753982544\n",
      "Epoch 82, CIFAR-10 Batch 3:  Accuracy: 0.4952000081539154 Loss: 0.9313724637031555\n",
      "Epoch 82, CIFAR-10 Batch 4:  Accuracy: 0.5063999891281128 Loss: 0.9982868432998657\n",
      "Epoch 82, CIFAR-10 Batch 5:  Accuracy: 0.5052000284194946 Loss: 1.2192186117172241\n",
      "Epoch 83, CIFAR-10 Batch 1:  Accuracy: 0.5023999810218811 Loss: 1.1270771026611328\n",
      "Epoch 83, CIFAR-10 Batch 2:  Accuracy: 0.49799999594688416 Loss: 1.1361273527145386\n",
      "Epoch 83, CIFAR-10 Batch 3:  Accuracy: 0.4968000054359436 Loss: 0.9449036717414856\n",
      "Epoch 83, CIFAR-10 Batch 4:  Accuracy: 0.510200023651123 Loss: 0.9682794809341431\n",
      "Epoch 83, CIFAR-10 Batch 5:  Accuracy: 0.5085999965667725 Loss: 1.16555917263031\n",
      "Epoch 84, CIFAR-10 Batch 1:  Accuracy: 0.5034000277519226 Loss: 1.0922634601593018\n",
      "Epoch 84, CIFAR-10 Batch 2:  Accuracy: 0.4986000061035156 Loss: 1.1502492427825928\n",
      "Epoch 84, CIFAR-10 Batch 3:  Accuracy: 0.5016000270843506 Loss: 0.9642036557197571\n",
      "Epoch 84, CIFAR-10 Batch 4:  Accuracy: 0.5072000026702881 Loss: 1.0372270345687866\n",
      "Epoch 84, CIFAR-10 Batch 5:  Accuracy: 0.5062000155448914 Loss: 1.1752570867538452\n",
      "Epoch 85, CIFAR-10 Batch 1:  Accuracy: 0.4975999891757965 Loss: 1.0882718563079834\n",
      "Epoch 85, CIFAR-10 Batch 2:  Accuracy: 0.5005999803543091 Loss: 1.1251161098480225\n",
      "Epoch 85, CIFAR-10 Batch 3:  Accuracy: 0.4966000020503998 Loss: 0.9526759386062622\n",
      "Epoch 85, CIFAR-10 Batch 4:  Accuracy: 0.5072000026702881 Loss: 0.9866998791694641\n",
      "Epoch 85, CIFAR-10 Batch 5:  Accuracy: 0.5049999952316284 Loss: 1.1588048934936523\n",
      "Epoch 86, CIFAR-10 Batch 1:  Accuracy: 0.4984000027179718 Loss: 1.0986582040786743\n",
      "Epoch 86, CIFAR-10 Batch 2:  Accuracy: 0.4975999891757965 Loss: 1.1048232316970825\n",
      "Epoch 86, CIFAR-10 Batch 3:  Accuracy: 0.49380001425743103 Loss: 0.9744483232498169\n",
      "Epoch 86, CIFAR-10 Batch 4:  Accuracy: 0.5072000026702881 Loss: 0.9799392819404602\n",
      "Epoch 86, CIFAR-10 Batch 5:  Accuracy: 0.5022000074386597 Loss: 1.167609453201294\n",
      "Epoch 87, CIFAR-10 Batch 1:  Accuracy: 0.5 Loss: 1.1089836359024048\n",
      "Epoch 87, CIFAR-10 Batch 2:  Accuracy: 0.4952000081539154 Loss: 1.104796051979065\n",
      "Epoch 87, CIFAR-10 Batch 3:  Accuracy: 0.4927999973297119 Loss: 0.9493820071220398\n",
      "Epoch 87, CIFAR-10 Batch 4:  Accuracy: 0.5081999897956848 Loss: 0.9808332324028015\n",
      "Epoch 87, CIFAR-10 Batch 5:  Accuracy: 0.5058000087738037 Loss: 1.1655542850494385\n",
      "Epoch 88, CIFAR-10 Batch 1:  Accuracy: 0.5004000067710876 Loss: 1.0857183933258057\n",
      "Epoch 88, CIFAR-10 Batch 2:  Accuracy: 0.5012000203132629 Loss: 1.1332224607467651\n",
      "Epoch 88, CIFAR-10 Batch 3:  Accuracy: 0.4952000081539154 Loss: 0.9361022114753723\n",
      "Epoch 88, CIFAR-10 Batch 4:  Accuracy: 0.503000020980835 Loss: 0.9771348834037781\n",
      "Epoch 88, CIFAR-10 Batch 5:  Accuracy: 0.5117999911308289 Loss: 1.156139850616455\n",
      "Epoch 89, CIFAR-10 Batch 1:  Accuracy: 0.5023999810218811 Loss: 1.063155174255371\n",
      "Epoch 89, CIFAR-10 Batch 2:  Accuracy: 0.4950000047683716 Loss: 1.1129183769226074\n",
      "Epoch 89, CIFAR-10 Batch 3:  Accuracy: 0.4957999885082245 Loss: 0.9383428692817688\n",
      "Epoch 89, CIFAR-10 Batch 4:  Accuracy: 0.5073999762535095 Loss: 0.9687185287475586\n",
      "Epoch 89, CIFAR-10 Batch 5:  Accuracy: 0.5080000162124634 Loss: 1.1344490051269531\n",
      "Epoch 90, CIFAR-10 Batch 1:  Accuracy: 0.4975999891757965 Loss: 1.0376276969909668\n",
      "Epoch 90, CIFAR-10 Batch 2:  Accuracy: 0.503600001335144 Loss: 1.1510158777236938\n",
      "Epoch 90, CIFAR-10 Batch 3:  Accuracy: 0.49900001287460327 Loss: 0.9352325201034546\n",
      "Epoch 90, CIFAR-10 Batch 4:  Accuracy: 0.510200023651123 Loss: 0.9593473672866821\n",
      "Epoch 90, CIFAR-10 Batch 5:  Accuracy: 0.5052000284194946 Loss: 1.1373076438903809\n",
      "Epoch 91, CIFAR-10 Batch 1:  Accuracy: 0.5004000067710876 Loss: 1.0234782695770264\n",
      "Epoch 91, CIFAR-10 Batch 2:  Accuracy: 0.5031999945640564 Loss: 1.1154382228851318\n",
      "Epoch 91, CIFAR-10 Batch 3:  Accuracy: 0.4941999912261963 Loss: 0.9132853746414185\n",
      "Epoch 91, CIFAR-10 Batch 4:  Accuracy: 0.508400022983551 Loss: 0.9811913371086121\n",
      "Epoch 91, CIFAR-10 Batch 5:  Accuracy: 0.5072000026702881 Loss: 1.1374717950820923\n",
      "Epoch 92, CIFAR-10 Batch 1:  Accuracy: 0.506600022315979 Loss: 1.0348471403121948\n",
      "Epoch 92, CIFAR-10 Batch 2:  Accuracy: 0.5022000074386597 Loss: 1.0759652853012085\n",
      "Epoch 92, CIFAR-10 Batch 3:  Accuracy: 0.5008000135421753 Loss: 0.9170228242874146\n",
      "Epoch 92, CIFAR-10 Batch 4:  Accuracy: 0.5095999836921692 Loss: 0.9760087728500366\n",
      "Epoch 92, CIFAR-10 Batch 5:  Accuracy: 0.5072000026702881 Loss: 1.171866774559021\n",
      "Epoch 93, CIFAR-10 Batch 1:  Accuracy: 0.49959999322891235 Loss: 1.0497853755950928\n",
      "Epoch 93, CIFAR-10 Batch 2:  Accuracy: 0.5016000270843506 Loss: 1.0791230201721191\n",
      "Epoch 93, CIFAR-10 Batch 3:  Accuracy: 0.49939998984336853 Loss: 0.9191781282424927\n",
      "Epoch 93, CIFAR-10 Batch 4:  Accuracy: 0.5081999897956848 Loss: 0.951133131980896\n",
      "Epoch 93, CIFAR-10 Batch 5:  Accuracy: 0.5059999823570251 Loss: 1.174691915512085\n",
      "Epoch 94, CIFAR-10 Batch 1:  Accuracy: 0.5016000270843506 Loss: 1.0298948287963867\n",
      "Epoch 94, CIFAR-10 Batch 2:  Accuracy: 0.5005999803543091 Loss: 1.0828616619110107\n",
      "Epoch 94, CIFAR-10 Batch 3:  Accuracy: 0.4984000027179718 Loss: 0.9232528805732727\n",
      "Epoch 94, CIFAR-10 Batch 4:  Accuracy: 0.5091999769210815 Loss: 0.9677755236625671\n",
      "Epoch 94, CIFAR-10 Batch 5:  Accuracy: 0.506600022315979 Loss: 1.129818320274353\n",
      "Epoch 95, CIFAR-10 Batch 1:  Accuracy: 0.5098000168800354 Loss: 1.0142217874526978\n",
      "Epoch 95, CIFAR-10 Batch 2:  Accuracy: 0.5041999816894531 Loss: 1.0784988403320312\n",
      "Epoch 95, CIFAR-10 Batch 3:  Accuracy: 0.5013999938964844 Loss: 0.9117040634155273\n",
      "Epoch 95, CIFAR-10 Batch 4:  Accuracy: 0.5105999708175659 Loss: 0.9847612380981445\n",
      "Epoch 95, CIFAR-10 Batch 5:  Accuracy: 0.5121999979019165 Loss: 1.1390094757080078\n",
      "Epoch 96, CIFAR-10 Batch 1:  Accuracy: 0.5009999871253967 Loss: 0.953034520149231\n",
      "Epoch 96, CIFAR-10 Batch 2:  Accuracy: 0.498199999332428 Loss: 1.1100842952728271\n",
      "Epoch 96, CIFAR-10 Batch 3:  Accuracy: 0.4991999864578247 Loss: 0.906533420085907\n",
      "Epoch 96, CIFAR-10 Batch 4:  Accuracy: 0.5098000168800354 Loss: 0.9766656756401062\n",
      "Epoch 96, CIFAR-10 Batch 5:  Accuracy: 0.501800000667572 Loss: 1.1380937099456787\n",
      "Epoch 97, CIFAR-10 Batch 1:  Accuracy: 0.5037999749183655 Loss: 0.9442712664604187\n",
      "Epoch 97, CIFAR-10 Batch 2:  Accuracy: 0.501800000667572 Loss: 1.0658810138702393\n",
      "Epoch 97, CIFAR-10 Batch 3:  Accuracy: 0.4991999864578247 Loss: 0.9094409942626953\n",
      "Epoch 97, CIFAR-10 Batch 4:  Accuracy: 0.5095999836921692 Loss: 0.9867414236068726\n",
      "Epoch 97, CIFAR-10 Batch 5:  Accuracy: 0.5070000290870667 Loss: 1.1466096639633179\n",
      "Epoch 98, CIFAR-10 Batch 1:  Accuracy: 0.506600022315979 Loss: 0.9554452896118164\n",
      "Epoch 98, CIFAR-10 Batch 2:  Accuracy: 0.5081999897956848 Loss: 1.0880632400512695\n",
      "Epoch 98, CIFAR-10 Batch 3:  Accuracy: 0.5005999803543091 Loss: 0.88621985912323\n",
      "Epoch 98, CIFAR-10 Batch 4:  Accuracy: 0.5070000290870667 Loss: 0.9826629757881165\n",
      "Epoch 98, CIFAR-10 Batch 5:  Accuracy: 0.5145999789237976 Loss: 1.1148250102996826\n",
      "Epoch 99, CIFAR-10 Batch 1:  Accuracy: 0.5088000297546387 Loss: 0.9669048190116882\n",
      "Epoch 99, CIFAR-10 Batch 2:  Accuracy: 0.5034000277519226 Loss: 1.036255121231079\n",
      "Epoch 99, CIFAR-10 Batch 3:  Accuracy: 0.5012000203132629 Loss: 0.9051724672317505\n",
      "Epoch 99, CIFAR-10 Batch 4:  Accuracy: 0.5103999972343445 Loss: 0.9926398992538452\n",
      "Epoch 99, CIFAR-10 Batch 5:  Accuracy: 0.5130000114440918 Loss: 1.1665159463882446\n",
      "Epoch 100, CIFAR-10 Batch 1:  Accuracy: 0.506600022315979 Loss: 0.9805759191513062\n",
      "Epoch 100, CIFAR-10 Batch 2:  Accuracy: 0.5131999850273132 Loss: 1.078418493270874\n",
      "Epoch 100, CIFAR-10 Batch 3:  Accuracy: 0.5037999749183655 Loss: 0.8962173461914062\n",
      "Epoch 100, CIFAR-10 Batch 4:  Accuracy: 0.5130000114440918 Loss: 0.9610985517501831\n",
      "Epoch 100, CIFAR-10 Batch 5:  Accuracy: 0.508400022983551 Loss: 1.1206566095352173\n",
      "Epoch 101, CIFAR-10 Batch 1:  Accuracy: 0.5072000026702881 Loss: 0.9418843984603882\n",
      "Epoch 101, CIFAR-10 Batch 2:  Accuracy: 0.5095999836921692 Loss: 1.0619909763336182\n",
      "Epoch 101, CIFAR-10 Batch 3:  Accuracy: 0.508400022983551 Loss: 0.8769410848617554\n",
      "Epoch 101, CIFAR-10 Batch 4:  Accuracy: 0.510200023651123 Loss: 0.9992669820785522\n",
      "Epoch 101, CIFAR-10 Batch 5:  Accuracy: 0.510200023651123 Loss: 1.107939600944519\n",
      "Epoch 102, CIFAR-10 Batch 1:  Accuracy: 0.5013999938964844 Loss: 0.9109455943107605\n",
      "Epoch 102, CIFAR-10 Batch 2:  Accuracy: 0.5099999904632568 Loss: 1.0854371786117554\n",
      "Epoch 102, CIFAR-10 Batch 3:  Accuracy: 0.5044000148773193 Loss: 0.8740386962890625\n",
      "Epoch 102, CIFAR-10 Batch 4:  Accuracy: 0.51419997215271 Loss: 0.963424563407898\n",
      "Epoch 102, CIFAR-10 Batch 5:  Accuracy: 0.5062000155448914 Loss: 1.0976366996765137\n",
      "Epoch 103, CIFAR-10 Batch 1:  Accuracy: 0.5077999830245972 Loss: 0.9930025339126587\n",
      "Epoch 103, CIFAR-10 Batch 2:  Accuracy: 0.5062000155448914 Loss: 1.0765798091888428\n",
      "Epoch 103, CIFAR-10 Batch 3:  Accuracy: 0.5113999843597412 Loss: 0.8573697805404663\n",
      "Epoch 103, CIFAR-10 Batch 4:  Accuracy: 0.5131999850273132 Loss: 0.9660404324531555\n",
      "Epoch 103, CIFAR-10 Batch 5:  Accuracy: 0.5157999992370605 Loss: 1.106711745262146\n",
      "Epoch 104, CIFAR-10 Batch 1:  Accuracy: 0.503600001335144 Loss: 0.94374018907547\n",
      "Epoch 104, CIFAR-10 Batch 2:  Accuracy: 0.5103999972343445 Loss: 1.0621044635772705\n",
      "Epoch 104, CIFAR-10 Batch 3:  Accuracy: 0.5023999810218811 Loss: 0.8909017443656921\n",
      "Epoch 104, CIFAR-10 Batch 4:  Accuracy: 0.5148000121116638 Loss: 0.9292534589767456\n",
      "Epoch 104, CIFAR-10 Batch 5:  Accuracy: 0.5138000249862671 Loss: 1.075573205947876\n",
      "Epoch 105, CIFAR-10 Batch 1:  Accuracy: 0.5099999904632568 Loss: 0.9021186828613281\n",
      "Epoch 105, CIFAR-10 Batch 2:  Accuracy: 0.5090000033378601 Loss: 1.0570393800735474\n",
      "Epoch 105, CIFAR-10 Batch 3:  Accuracy: 0.49939998984336853 Loss: 0.858348548412323\n",
      "Epoch 105, CIFAR-10 Batch 4:  Accuracy: 0.5175999999046326 Loss: 0.9419490694999695\n",
      "Epoch 105, CIFAR-10 Batch 5:  Accuracy: 0.5112000107765198 Loss: 1.0384117364883423\n",
      "Epoch 106, CIFAR-10 Batch 1:  Accuracy: 0.5126000046730042 Loss: 0.9244773983955383\n",
      "Epoch 106, CIFAR-10 Batch 2:  Accuracy: 0.5045999884605408 Loss: 1.0167285203933716\n",
      "Epoch 106, CIFAR-10 Batch 3:  Accuracy: 0.5090000033378601 Loss: 0.8710787892341614\n",
      "Epoch 106, CIFAR-10 Batch 4:  Accuracy: 0.5123999714851379 Loss: 0.9655941724777222\n",
      "Epoch 106, CIFAR-10 Batch 5:  Accuracy: 0.5134000182151794 Loss: 1.0609006881713867\n",
      "Epoch 107, CIFAR-10 Batch 1:  Accuracy: 0.5090000033378601 Loss: 0.8788407444953918\n",
      "Epoch 107, CIFAR-10 Batch 2:  Accuracy: 0.5095999836921692 Loss: 1.0528329610824585\n",
      "Epoch 107, CIFAR-10 Batch 3:  Accuracy: 0.5052000284194946 Loss: 0.8667054176330566\n",
      "Epoch 107, CIFAR-10 Batch 4:  Accuracy: 0.5123999714851379 Loss: 0.9326380491256714\n",
      "Epoch 107, CIFAR-10 Batch 5:  Accuracy: 0.5090000033378601 Loss: 1.073993444442749\n",
      "Epoch 108, CIFAR-10 Batch 1:  Accuracy: 0.5073999762535095 Loss: 0.9241229891777039\n",
      "Epoch 108, CIFAR-10 Batch 2:  Accuracy: 0.5081999897956848 Loss: 1.028519868850708\n",
      "Epoch 108, CIFAR-10 Batch 3:  Accuracy: 0.5040000081062317 Loss: 0.8266445398330688\n",
      "Epoch 108, CIFAR-10 Batch 4:  Accuracy: 0.5134000182151794 Loss: 0.9700323343276978\n",
      "Epoch 108, CIFAR-10 Batch 5:  Accuracy: 0.5120000243186951 Loss: 1.0546722412109375\n",
      "Epoch 109, CIFAR-10 Batch 1:  Accuracy: 0.5045999884605408 Loss: 0.9226687550544739\n",
      "Epoch 109, CIFAR-10 Batch 2:  Accuracy: 0.5055999755859375 Loss: 1.017291784286499\n",
      "Epoch 109, CIFAR-10 Batch 3:  Accuracy: 0.5052000284194946 Loss: 0.8134773373603821\n",
      "Epoch 109, CIFAR-10 Batch 4:  Accuracy: 0.5120000243186951 Loss: 0.9440058469772339\n",
      "Epoch 109, CIFAR-10 Batch 5:  Accuracy: 0.51419997215271 Loss: 1.0768792629241943\n",
      "Epoch 110, CIFAR-10 Batch 1:  Accuracy: 0.5121999979019165 Loss: 0.902342677116394\n",
      "Epoch 110, CIFAR-10 Batch 2:  Accuracy: 0.5156000256538391 Loss: 1.0363731384277344\n",
      "Epoch 110, CIFAR-10 Batch 3:  Accuracy: 0.5091999769210815 Loss: 0.8088515400886536\n",
      "Epoch 110, CIFAR-10 Batch 4:  Accuracy: 0.517799973487854 Loss: 0.9118822813034058\n",
      "Epoch 110, CIFAR-10 Batch 5:  Accuracy: 0.5112000107765198 Loss: 1.1044137477874756\n",
      "Epoch 111, CIFAR-10 Batch 1:  Accuracy: 0.510200023651123 Loss: 0.9045016169548035\n",
      "Epoch 111, CIFAR-10 Batch 2:  Accuracy: 0.5131999850273132 Loss: 1.0422227382659912\n",
      "Epoch 111, CIFAR-10 Batch 3:  Accuracy: 0.506600022315979 Loss: 0.7834503650665283\n",
      "Epoch 111, CIFAR-10 Batch 4:  Accuracy: 0.5162000060081482 Loss: 0.9253463745117188\n",
      "Epoch 111, CIFAR-10 Batch 5:  Accuracy: 0.5112000107765198 Loss: 1.0859825611114502\n",
      "Epoch 112, CIFAR-10 Batch 1:  Accuracy: 0.5098000168800354 Loss: 0.8771152496337891\n",
      "Epoch 112, CIFAR-10 Batch 2:  Accuracy: 0.5148000121116638 Loss: 1.0446630716323853\n",
      "Epoch 112, CIFAR-10 Batch 3:  Accuracy: 0.5088000297546387 Loss: 0.8246345520019531\n",
      "Epoch 112, CIFAR-10 Batch 4:  Accuracy: 0.5145999789237976 Loss: 0.9384962320327759\n",
      "Epoch 112, CIFAR-10 Batch 5:  Accuracy: 0.5180000066757202 Loss: 1.0905427932739258\n",
      "Epoch 113, CIFAR-10 Batch 1:  Accuracy: 0.5109999775886536 Loss: 0.8801536560058594\n",
      "Epoch 113, CIFAR-10 Batch 2:  Accuracy: 0.5112000107765198 Loss: 1.040785551071167\n",
      "Epoch 113, CIFAR-10 Batch 3:  Accuracy: 0.5148000121116638 Loss: 0.8264349102973938\n",
      "Epoch 113, CIFAR-10 Batch 4:  Accuracy: 0.5171999931335449 Loss: 0.9169812202453613\n",
      "Epoch 113, CIFAR-10 Batch 5:  Accuracy: 0.5153999924659729 Loss: 1.0961027145385742\n",
      "Epoch 114, CIFAR-10 Batch 1:  Accuracy: 0.5120000243186951 Loss: 0.8496166467666626\n",
      "Epoch 114, CIFAR-10 Batch 2:  Accuracy: 0.515999972820282 Loss: 0.9995690584182739\n",
      "Epoch 114, CIFAR-10 Batch 3:  Accuracy: 0.5108000040054321 Loss: 0.8227396011352539\n",
      "Epoch 114, CIFAR-10 Batch 4:  Accuracy: 0.5180000066757202 Loss: 0.9282329678535461\n",
      "Epoch 114, CIFAR-10 Batch 5:  Accuracy: 0.5120000243186951 Loss: 1.0738723278045654\n",
      "Epoch 115, CIFAR-10 Batch 1:  Accuracy: 0.5109999775886536 Loss: 0.8473661541938782\n",
      "Epoch 115, CIFAR-10 Batch 2:  Accuracy: 0.5167999863624573 Loss: 1.0214006900787354\n",
      "Epoch 115, CIFAR-10 Batch 3:  Accuracy: 0.5109999775886536 Loss: 0.8104997873306274\n",
      "Epoch 115, CIFAR-10 Batch 4:  Accuracy: 0.5210000276565552 Loss: 0.9096973538398743\n",
      "Epoch 115, CIFAR-10 Batch 5:  Accuracy: 0.5139999985694885 Loss: 1.110522985458374\n",
      "Epoch 116, CIFAR-10 Batch 1:  Accuracy: 0.5144000053405762 Loss: 0.869289219379425\n",
      "Epoch 116, CIFAR-10 Batch 2:  Accuracy: 0.5085999965667725 Loss: 1.0390911102294922\n",
      "Epoch 116, CIFAR-10 Batch 3:  Accuracy: 0.5113999843597412 Loss: 0.779105007648468\n",
      "Epoch 116, CIFAR-10 Batch 4:  Accuracy: 0.5135999917984009 Loss: 0.930648684501648\n",
      "Epoch 116, CIFAR-10 Batch 5:  Accuracy: 0.5135999917984009 Loss: 1.0885136127471924\n",
      "Epoch 117, CIFAR-10 Batch 1:  Accuracy: 0.5171999931335449 Loss: 0.8892512321472168\n",
      "Epoch 117, CIFAR-10 Batch 2:  Accuracy: 0.520799994468689 Loss: 1.05788254737854\n",
      "Epoch 117, CIFAR-10 Batch 3:  Accuracy: 0.5167999863624573 Loss: 0.7649323344230652\n",
      "Epoch 117, CIFAR-10 Batch 4:  Accuracy: 0.5202000141143799 Loss: 0.980760931968689\n",
      "Epoch 117, CIFAR-10 Batch 5:  Accuracy: 0.5149999856948853 Loss: 1.103206753730774\n",
      "Epoch 118, CIFAR-10 Batch 1:  Accuracy: 0.5145999789237976 Loss: 0.8871133923530579\n",
      "Epoch 118, CIFAR-10 Batch 2:  Accuracy: 0.5121999979019165 Loss: 1.0425825119018555\n",
      "Epoch 118, CIFAR-10 Batch 3:  Accuracy: 0.5109999775886536 Loss: 0.7736152410507202\n",
      "Epoch 118, CIFAR-10 Batch 4:  Accuracy: 0.5228000283241272 Loss: 0.8942652940750122\n",
      "Epoch 118, CIFAR-10 Batch 5:  Accuracy: 0.51419997215271 Loss: 1.0869567394256592\n",
      "Epoch 119, CIFAR-10 Batch 1:  Accuracy: 0.5134000182151794 Loss: 0.8702682256698608\n",
      "Epoch 119, CIFAR-10 Batch 2:  Accuracy: 0.5095999836921692 Loss: 1.044711709022522\n",
      "Epoch 119, CIFAR-10 Batch 3:  Accuracy: 0.5126000046730042 Loss: 0.8079706430435181\n",
      "Epoch 119, CIFAR-10 Batch 4:  Accuracy: 0.5192000269889832 Loss: 0.9123202562332153\n",
      "Epoch 119, CIFAR-10 Batch 5:  Accuracy: 0.5099999904632568 Loss: 1.1085551977157593\n",
      "Epoch 120, CIFAR-10 Batch 1:  Accuracy: 0.5167999863624573 Loss: 0.8393443822860718\n",
      "Epoch 120, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 1.0180270671844482\n",
      "Epoch 120, CIFAR-10 Batch 3:  Accuracy: 0.5088000297546387 Loss: 0.7764579057693481\n",
      "Epoch 120, CIFAR-10 Batch 4:  Accuracy: 0.5145999789237976 Loss: 0.9008239507675171\n",
      "Epoch 120, CIFAR-10 Batch 5:  Accuracy: 0.5126000046730042 Loss: 1.0968296527862549\n",
      "Epoch 121, CIFAR-10 Batch 1:  Accuracy: 0.5117999911308289 Loss: 0.858661949634552\n",
      "Epoch 121, CIFAR-10 Batch 2:  Accuracy: 0.5149999856948853 Loss: 1.0304034948349\n",
      "Epoch 121, CIFAR-10 Batch 3:  Accuracy: 0.5156000256538391 Loss: 0.7598744034767151\n",
      "Epoch 121, CIFAR-10 Batch 4:  Accuracy: 0.5157999992370605 Loss: 0.9329948425292969\n",
      "Epoch 121, CIFAR-10 Batch 5:  Accuracy: 0.5181999802589417 Loss: 1.0779927968978882\n",
      "Epoch 122, CIFAR-10 Batch 1:  Accuracy: 0.5126000046730042 Loss: 0.8539034724235535\n",
      "Epoch 122, CIFAR-10 Batch 2:  Accuracy: 0.5175999999046326 Loss: 1.0460718870162964\n",
      "Epoch 122, CIFAR-10 Batch 3:  Accuracy: 0.5131999850273132 Loss: 0.7757359743118286\n",
      "Epoch 122, CIFAR-10 Batch 4:  Accuracy: 0.5194000005722046 Loss: 0.8652315139770508\n",
      "Epoch 122, CIFAR-10 Batch 5:  Accuracy: 0.5131999850273132 Loss: 1.04025399684906\n",
      "Epoch 123, CIFAR-10 Batch 1:  Accuracy: 0.5112000107765198 Loss: 0.8556817173957825\n",
      "Epoch 123, CIFAR-10 Batch 2:  Accuracy: 0.5174000263214111 Loss: 1.0617969036102295\n",
      "Epoch 123, CIFAR-10 Batch 3:  Accuracy: 0.5120000243186951 Loss: 0.7849829196929932\n",
      "Epoch 123, CIFAR-10 Batch 4:  Accuracy: 0.5202000141143799 Loss: 0.8888800740242004\n",
      "Epoch 123, CIFAR-10 Batch 5:  Accuracy: 0.5123999714851379 Loss: 1.072490930557251\n",
      "Epoch 124, CIFAR-10 Batch 1:  Accuracy: 0.517799973487854 Loss: 0.8512647747993469\n",
      "Epoch 124, CIFAR-10 Batch 2:  Accuracy: 0.5189999938011169 Loss: 1.047715425491333\n",
      "Epoch 124, CIFAR-10 Batch 3:  Accuracy: 0.5184000134468079 Loss: 0.7912644743919373\n",
      "Epoch 124, CIFAR-10 Batch 4:  Accuracy: 0.5216000080108643 Loss: 0.8553192019462585\n",
      "Epoch 124, CIFAR-10 Batch 5:  Accuracy: 0.5139999985694885 Loss: 1.0684653520584106\n",
      "Epoch 125, CIFAR-10 Batch 1:  Accuracy: 0.5130000114440918 Loss: 0.8422250747680664\n",
      "Epoch 125, CIFAR-10 Batch 2:  Accuracy: 0.5194000005722046 Loss: 1.0415791273117065\n",
      "Epoch 125, CIFAR-10 Batch 3:  Accuracy: 0.51419997215271 Loss: 0.7529051899909973\n",
      "Epoch 125, CIFAR-10 Batch 4:  Accuracy: 0.5230000019073486 Loss: 0.8846062421798706\n",
      "Epoch 125, CIFAR-10 Batch 5:  Accuracy: 0.5217999815940857 Loss: 1.0471464395523071\n",
      "Epoch 126, CIFAR-10 Batch 1:  Accuracy: 0.5095999836921692 Loss: 0.8524184226989746\n",
      "Epoch 126, CIFAR-10 Batch 2:  Accuracy: 0.520799994468689 Loss: 1.042404055595398\n",
      "Epoch 126, CIFAR-10 Batch 3:  Accuracy: 0.5166000127792358 Loss: 0.7794982194900513\n",
      "Epoch 126, CIFAR-10 Batch 4:  Accuracy: 0.519599974155426 Loss: 0.8786239624023438\n",
      "Epoch 126, CIFAR-10 Batch 5:  Accuracy: 0.520799994468689 Loss: 1.055361270904541\n",
      "Epoch 127, CIFAR-10 Batch 1:  Accuracy: 0.5098000168800354 Loss: 0.8234497308731079\n",
      "Epoch 127, CIFAR-10 Batch 2:  Accuracy: 0.5189999938011169 Loss: 1.0347639322280884\n",
      "Epoch 127, CIFAR-10 Batch 3:  Accuracy: 0.5126000046730042 Loss: 0.7183613181114197\n",
      "Epoch 127, CIFAR-10 Batch 4:  Accuracy: 0.5220000147819519 Loss: 0.8836304545402527\n",
      "Epoch 127, CIFAR-10 Batch 5:  Accuracy: 0.5210000276565552 Loss: 1.0596399307250977\n",
      "Epoch 128, CIFAR-10 Batch 1:  Accuracy: 0.5139999985694885 Loss: 0.795552134513855\n",
      "Epoch 128, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 1.041564702987671\n",
      "Epoch 128, CIFAR-10 Batch 3:  Accuracy: 0.5121999979019165 Loss: 0.7628546953201294\n",
      "Epoch 128, CIFAR-10 Batch 4:  Accuracy: 0.5230000019073486 Loss: 0.8511369824409485\n",
      "Epoch 128, CIFAR-10 Batch 5:  Accuracy: 0.5220000147819519 Loss: 1.089874267578125\n",
      "Epoch 129, CIFAR-10 Batch 1:  Accuracy: 0.517799973487854 Loss: 0.8305851221084595\n",
      "Epoch 129, CIFAR-10 Batch 2:  Accuracy: 0.5175999999046326 Loss: 1.0017043352127075\n",
      "Epoch 129, CIFAR-10 Batch 3:  Accuracy: 0.5112000107765198 Loss: 0.7435064315795898\n",
      "Epoch 129, CIFAR-10 Batch 4:  Accuracy: 0.5148000121116638 Loss: 0.8940397500991821\n",
      "Epoch 129, CIFAR-10 Batch 5:  Accuracy: 0.5149999856948853 Loss: 1.035115122795105\n",
      "Epoch 130, CIFAR-10 Batch 1:  Accuracy: 0.5175999999046326 Loss: 0.8062458038330078\n",
      "Epoch 130, CIFAR-10 Batch 2:  Accuracy: 0.515999972820282 Loss: 1.0160704851150513\n",
      "Epoch 130, CIFAR-10 Batch 3:  Accuracy: 0.5139999985694885 Loss: 0.7608290910720825\n",
      "Epoch 130, CIFAR-10 Batch 4:  Accuracy: 0.5224000215530396 Loss: 0.832200825214386\n",
      "Epoch 130, CIFAR-10 Batch 5:  Accuracy: 0.5224000215530396 Loss: 1.01100754737854\n",
      "Epoch 131, CIFAR-10 Batch 1:  Accuracy: 0.5112000107765198 Loss: 0.8158620595932007\n",
      "Epoch 131, CIFAR-10 Batch 2:  Accuracy: 0.5220000147819519 Loss: 1.000145673751831\n",
      "Epoch 131, CIFAR-10 Batch 3:  Accuracy: 0.5148000121116638 Loss: 0.7359102964401245\n",
      "Epoch 131, CIFAR-10 Batch 4:  Accuracy: 0.5216000080108643 Loss: 0.8342393636703491\n",
      "Epoch 131, CIFAR-10 Batch 5:  Accuracy: 0.5189999938011169 Loss: 1.0193895101547241\n",
      "Epoch 132, CIFAR-10 Batch 1:  Accuracy: 0.5116000175476074 Loss: 0.7885898351669312\n",
      "Epoch 132, CIFAR-10 Batch 2:  Accuracy: 0.5220000147819519 Loss: 0.9996076822280884\n",
      "Epoch 132, CIFAR-10 Batch 3:  Accuracy: 0.5109999775886536 Loss: 0.7501104474067688\n",
      "Epoch 132, CIFAR-10 Batch 4:  Accuracy: 0.5184000134468079 Loss: 0.7808672785758972\n",
      "Epoch 132, CIFAR-10 Batch 5:  Accuracy: 0.5234000086784363 Loss: 1.0251333713531494\n",
      "Epoch 133, CIFAR-10 Batch 1:  Accuracy: 0.5199999809265137 Loss: 0.7988907098770142\n",
      "Epoch 133, CIFAR-10 Batch 2:  Accuracy: 0.5198000073432922 Loss: 1.0189487934112549\n",
      "Epoch 133, CIFAR-10 Batch 3:  Accuracy: 0.5171999931335449 Loss: 0.7450671195983887\n",
      "Epoch 133, CIFAR-10 Batch 4:  Accuracy: 0.5221999883651733 Loss: 0.8367452621459961\n",
      "Epoch 133, CIFAR-10 Batch 5:  Accuracy: 0.5157999992370605 Loss: 1.006677269935608\n",
      "Epoch 134, CIFAR-10 Batch 1:  Accuracy: 0.5180000066757202 Loss: 0.7736888527870178\n",
      "Epoch 134, CIFAR-10 Batch 2:  Accuracy: 0.5131999850273132 Loss: 1.0273021459579468\n",
      "Epoch 134, CIFAR-10 Batch 3:  Accuracy: 0.5194000005722046 Loss: 0.7940593957901001\n",
      "Epoch 134, CIFAR-10 Batch 4:  Accuracy: 0.5212000012397766 Loss: 0.7884010076522827\n",
      "Epoch 134, CIFAR-10 Batch 5:  Accuracy: 0.5189999938011169 Loss: 1.0341360569000244\n",
      "Epoch 135, CIFAR-10 Batch 1:  Accuracy: 0.5228000283241272 Loss: 0.7785248160362244\n",
      "Epoch 135, CIFAR-10 Batch 2:  Accuracy: 0.5170000195503235 Loss: 1.053378701210022\n",
      "Epoch 135, CIFAR-10 Batch 3:  Accuracy: 0.5174000263214111 Loss: 0.7268782258033752\n",
      "Epoch 135, CIFAR-10 Batch 4:  Accuracy: 0.5267999768257141 Loss: 0.7925901412963867\n",
      "Epoch 135, CIFAR-10 Batch 5:  Accuracy: 0.5253999829292297 Loss: 1.0626351833343506\n",
      "Epoch 136, CIFAR-10 Batch 1:  Accuracy: 0.5181999802589417 Loss: 0.7538817524909973\n",
      "Epoch 136, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 1.0034841299057007\n",
      "Epoch 136, CIFAR-10 Batch 3:  Accuracy: 0.5167999863624573 Loss: 0.7501880526542664\n",
      "Epoch 136, CIFAR-10 Batch 4:  Accuracy: 0.5212000012397766 Loss: 0.7713307738304138\n",
      "Epoch 136, CIFAR-10 Batch 5:  Accuracy: 0.5189999938011169 Loss: 1.0316299200057983\n",
      "Epoch 137, CIFAR-10 Batch 1:  Accuracy: 0.5180000066757202 Loss: 0.7785011529922485\n",
      "Epoch 137, CIFAR-10 Batch 2:  Accuracy: 0.5220000147819519 Loss: 0.9757099151611328\n",
      "Epoch 137, CIFAR-10 Batch 3:  Accuracy: 0.5181999802589417 Loss: 0.7639837265014648\n",
      "Epoch 137, CIFAR-10 Batch 4:  Accuracy: 0.5175999999046326 Loss: 0.7452681064605713\n",
      "Epoch 137, CIFAR-10 Batch 5:  Accuracy: 0.5220000147819519 Loss: 1.0047781467437744\n",
      "Epoch 138, CIFAR-10 Batch 1:  Accuracy: 0.5127999782562256 Loss: 0.7790249586105347\n",
      "Epoch 138, CIFAR-10 Batch 2:  Accuracy: 0.5199999809265137 Loss: 0.9868776202201843\n",
      "Epoch 138, CIFAR-10 Batch 3:  Accuracy: 0.5163999795913696 Loss: 0.7613482475280762\n",
      "Epoch 138, CIFAR-10 Batch 4:  Accuracy: 0.520799994468689 Loss: 0.7719359993934631\n",
      "Epoch 138, CIFAR-10 Batch 5:  Accuracy: 0.5184000134468079 Loss: 1.0228259563446045\n",
      "Epoch 139, CIFAR-10 Batch 1:  Accuracy: 0.5174000263214111 Loss: 0.8425590395927429\n",
      "Epoch 139, CIFAR-10 Batch 2:  Accuracy: 0.5157999992370605 Loss: 1.0290175676345825\n",
      "Epoch 139, CIFAR-10 Batch 3:  Accuracy: 0.5162000060081482 Loss: 0.7422459125518799\n",
      "Epoch 139, CIFAR-10 Batch 4:  Accuracy: 0.524399995803833 Loss: 0.7860280275344849\n",
      "Epoch 139, CIFAR-10 Batch 5:  Accuracy: 0.5221999883651733 Loss: 1.0426760911941528\n",
      "Epoch 140, CIFAR-10 Batch 1:  Accuracy: 0.5188000202178955 Loss: 0.8270391225814819\n",
      "Epoch 140, CIFAR-10 Batch 2:  Accuracy: 0.5198000073432922 Loss: 0.983454704284668\n",
      "Epoch 140, CIFAR-10 Batch 3:  Accuracy: 0.5156000256538391 Loss: 0.7069670557975769\n",
      "Epoch 140, CIFAR-10 Batch 4:  Accuracy: 0.517799973487854 Loss: 0.8112173080444336\n",
      "Epoch 140, CIFAR-10 Batch 5:  Accuracy: 0.520799994468689 Loss: 1.0003608465194702\n",
      "Epoch 141, CIFAR-10 Batch 1:  Accuracy: 0.5163999795913696 Loss: 0.8111178278923035\n",
      "Epoch 141, CIFAR-10 Batch 2:  Accuracy: 0.5202000141143799 Loss: 1.0110527276992798\n",
      "Epoch 141, CIFAR-10 Batch 3:  Accuracy: 0.5170000195503235 Loss: 0.7366928458213806\n",
      "Epoch 141, CIFAR-10 Batch 4:  Accuracy: 0.525600016117096 Loss: 0.8147329092025757\n",
      "Epoch 141, CIFAR-10 Batch 5:  Accuracy: 0.5224000215530396 Loss: 1.024234652519226\n",
      "Epoch 142, CIFAR-10 Batch 1:  Accuracy: 0.5174000263214111 Loss: 0.8037562370300293\n",
      "Epoch 142, CIFAR-10 Batch 2:  Accuracy: 0.5278000235557556 Loss: 0.9949128031730652\n",
      "Epoch 142, CIFAR-10 Batch 3:  Accuracy: 0.51419997215271 Loss: 0.7413471937179565\n",
      "Epoch 142, CIFAR-10 Batch 4:  Accuracy: 0.5220000147819519 Loss: 0.7893657684326172\n",
      "Epoch 142, CIFAR-10 Batch 5:  Accuracy: 0.5185999870300293 Loss: 1.0056625604629517\n",
      "Epoch 143, CIFAR-10 Batch 1:  Accuracy: 0.5174000263214111 Loss: 0.8099228143692017\n",
      "Epoch 143, CIFAR-10 Batch 2:  Accuracy: 0.5167999863624573 Loss: 1.0294411182403564\n",
      "Epoch 143, CIFAR-10 Batch 3:  Accuracy: 0.517799973487854 Loss: 0.7538296580314636\n",
      "Epoch 143, CIFAR-10 Batch 4:  Accuracy: 0.5249999761581421 Loss: 0.7557910680770874\n",
      "Epoch 143, CIFAR-10 Batch 5:  Accuracy: 0.5238000154495239 Loss: 1.002623200416565\n",
      "Epoch 144, CIFAR-10 Batch 1:  Accuracy: 0.5138000249862671 Loss: 0.7916995286941528\n",
      "Epoch 144, CIFAR-10 Batch 2:  Accuracy: 0.5145999789237976 Loss: 1.006569504737854\n",
      "Epoch 144, CIFAR-10 Batch 3:  Accuracy: 0.521399974822998 Loss: 0.7762022018432617\n",
      "Epoch 144, CIFAR-10 Batch 4:  Accuracy: 0.5264000296592712 Loss: 0.7683333158493042\n",
      "Epoch 144, CIFAR-10 Batch 5:  Accuracy: 0.5253999829292297 Loss: 1.0013712644577026\n",
      "Epoch 145, CIFAR-10 Batch 1:  Accuracy: 0.5185999870300293 Loss: 0.7657540440559387\n",
      "Epoch 145, CIFAR-10 Batch 2:  Accuracy: 0.5175999999046326 Loss: 1.041050910949707\n",
      "Epoch 145, CIFAR-10 Batch 3:  Accuracy: 0.5206000208854675 Loss: 0.7621564865112305\n",
      "Epoch 145, CIFAR-10 Batch 4:  Accuracy: 0.5216000080108643 Loss: 0.7691380977630615\n",
      "Epoch 145, CIFAR-10 Batch 5:  Accuracy: 0.5299999713897705 Loss: 1.0019822120666504\n",
      "Epoch 146, CIFAR-10 Batch 1:  Accuracy: 0.5216000080108643 Loss: 0.7961493134498596\n",
      "Epoch 146, CIFAR-10 Batch 2:  Accuracy: 0.5189999938011169 Loss: 1.0153605937957764\n",
      "Epoch 146, CIFAR-10 Batch 3:  Accuracy: 0.517799973487854 Loss: 0.7481820583343506\n",
      "Epoch 146, CIFAR-10 Batch 4:  Accuracy: 0.5257999897003174 Loss: 0.7762255072593689\n",
      "Epoch 146, CIFAR-10 Batch 5:  Accuracy: 0.524399995803833 Loss: 1.0069963932037354\n",
      "Epoch 147, CIFAR-10 Batch 1:  Accuracy: 0.5217999815940857 Loss: 0.7821069955825806\n",
      "Epoch 147, CIFAR-10 Batch 2:  Accuracy: 0.5210000276565552 Loss: 0.9919973611831665\n",
      "Epoch 147, CIFAR-10 Batch 3:  Accuracy: 0.5167999863624573 Loss: 0.7415647506713867\n",
      "Epoch 147, CIFAR-10 Batch 4:  Accuracy: 0.5271999835968018 Loss: 0.8235095739364624\n",
      "Epoch 147, CIFAR-10 Batch 5:  Accuracy: 0.526199996471405 Loss: 0.9800969958305359\n",
      "Epoch 148, CIFAR-10 Batch 1:  Accuracy: 0.5126000046730042 Loss: 0.8205183148384094\n",
      "Epoch 148, CIFAR-10 Batch 2:  Accuracy: 0.51419997215271 Loss: 1.0208568572998047\n",
      "Epoch 148, CIFAR-10 Batch 3:  Accuracy: 0.515999972820282 Loss: 0.730233371257782\n",
      "Epoch 148, CIFAR-10 Batch 4:  Accuracy: 0.5220000147819519 Loss: 0.7789284586906433\n",
      "Epoch 148, CIFAR-10 Batch 5:  Accuracy: 0.5278000235557556 Loss: 1.052119493484497\n",
      "Epoch 149, CIFAR-10 Batch 1:  Accuracy: 0.5216000080108643 Loss: 0.7843660712242126\n",
      "Epoch 149, CIFAR-10 Batch 2:  Accuracy: 0.5198000073432922 Loss: 1.0192184448242188\n",
      "Epoch 149, CIFAR-10 Batch 3:  Accuracy: 0.5206000208854675 Loss: 0.7584931254386902\n",
      "Epoch 149, CIFAR-10 Batch 4:  Accuracy: 0.5212000012397766 Loss: 0.7961829304695129\n",
      "Epoch 149, CIFAR-10 Batch 5:  Accuracy: 0.5234000086784363 Loss: 0.9806968569755554\n",
      "Epoch 150, CIFAR-10 Batch 1:  Accuracy: 0.5184000134468079 Loss: 0.7861618995666504\n",
      "Epoch 150, CIFAR-10 Batch 2:  Accuracy: 0.5185999870300293 Loss: 1.0227638483047485\n",
      "Epoch 150, CIFAR-10 Batch 3:  Accuracy: 0.5184000134468079 Loss: 0.6974750757217407\n",
      "Epoch 150, CIFAR-10 Batch 4:  Accuracy: 0.5221999883651733 Loss: 0.7467012405395508\n",
      "Epoch 150, CIFAR-10 Batch 5:  Accuracy: 0.5260000228881836 Loss: 0.9869699478149414\n",
      "Epoch 151, CIFAR-10 Batch 1:  Accuracy: 0.5202000141143799 Loss: 0.7908239364624023\n",
      "Epoch 151, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 1.0095422267913818\n",
      "Epoch 151, CIFAR-10 Batch 3:  Accuracy: 0.517799973487854 Loss: 0.743569016456604\n",
      "Epoch 151, CIFAR-10 Batch 4:  Accuracy: 0.5239999890327454 Loss: 0.7815809845924377\n",
      "Epoch 151, CIFAR-10 Batch 5:  Accuracy: 0.5293999910354614 Loss: 0.9706012606620789\n",
      "Epoch 152, CIFAR-10 Batch 1:  Accuracy: 0.519599974155426 Loss: 0.7881149053573608\n",
      "Epoch 152, CIFAR-10 Batch 2:  Accuracy: 0.5184000134468079 Loss: 1.0098234415054321\n",
      "Epoch 152, CIFAR-10 Batch 3:  Accuracy: 0.5166000127792358 Loss: 0.7136362791061401\n",
      "Epoch 152, CIFAR-10 Batch 4:  Accuracy: 0.5260000228881836 Loss: 0.7415790557861328\n",
      "Epoch 152, CIFAR-10 Batch 5:  Accuracy: 0.5320000052452087 Loss: 0.992212176322937\n",
      "Epoch 153, CIFAR-10 Batch 1:  Accuracy: 0.5217999815940857 Loss: 0.8073011636734009\n",
      "Epoch 153, CIFAR-10 Batch 2:  Accuracy: 0.5192000269889832 Loss: 1.0245914459228516\n",
      "Epoch 153, CIFAR-10 Batch 3:  Accuracy: 0.5212000012397766 Loss: 0.7088367938995361\n",
      "Epoch 153, CIFAR-10 Batch 4:  Accuracy: 0.5297999978065491 Loss: 0.7698691487312317\n",
      "Epoch 153, CIFAR-10 Batch 5:  Accuracy: 0.5317999720573425 Loss: 0.9475294351577759\n",
      "Epoch 154, CIFAR-10 Batch 1:  Accuracy: 0.5267999768257141 Loss: 0.7981122732162476\n",
      "Epoch 154, CIFAR-10 Batch 2:  Accuracy: 0.520799994468689 Loss: 1.0092966556549072\n",
      "Epoch 154, CIFAR-10 Batch 3:  Accuracy: 0.5210000276565552 Loss: 0.7170742154121399\n",
      "Epoch 154, CIFAR-10 Batch 4:  Accuracy: 0.5260000228881836 Loss: 0.7249093055725098\n",
      "Epoch 154, CIFAR-10 Batch 5:  Accuracy: 0.5303999781608582 Loss: 1.0354021787643433\n",
      "Epoch 155, CIFAR-10 Batch 1:  Accuracy: 0.5266000032424927 Loss: 0.7412293553352356\n",
      "Epoch 155, CIFAR-10 Batch 2:  Accuracy: 0.5192000269889832 Loss: 1.055121660232544\n",
      "Epoch 155, CIFAR-10 Batch 3:  Accuracy: 0.5257999897003174 Loss: 0.6998380422592163\n",
      "Epoch 155, CIFAR-10 Batch 4:  Accuracy: 0.5231999754905701 Loss: 0.7469415068626404\n",
      "Epoch 155, CIFAR-10 Batch 5:  Accuracy: 0.5307999849319458 Loss: 1.008787989616394\n",
      "Epoch 156, CIFAR-10 Batch 1:  Accuracy: 0.520799994468689 Loss: 0.8145941495895386\n",
      "Epoch 156, CIFAR-10 Batch 2:  Accuracy: 0.5228000283241272 Loss: 1.0323721170425415\n",
      "Epoch 156, CIFAR-10 Batch 3:  Accuracy: 0.5175999999046326 Loss: 0.6945197582244873\n",
      "Epoch 156, CIFAR-10 Batch 4:  Accuracy: 0.5253999829292297 Loss: 0.7296940088272095\n",
      "Epoch 156, CIFAR-10 Batch 5:  Accuracy: 0.527999997138977 Loss: 1.0058324337005615\n",
      "Epoch 157, CIFAR-10 Batch 1:  Accuracy: 0.5270000100135803 Loss: 0.7596047520637512\n",
      "Epoch 157, CIFAR-10 Batch 2:  Accuracy: 0.5239999890327454 Loss: 1.0599615573883057\n",
      "Epoch 157, CIFAR-10 Batch 3:  Accuracy: 0.5174000263214111 Loss: 0.7224150896072388\n",
      "Epoch 157, CIFAR-10 Batch 4:  Accuracy: 0.5246000289916992 Loss: 0.744749903678894\n",
      "Epoch 157, CIFAR-10 Batch 5:  Accuracy: 0.5342000126838684 Loss: 1.0133914947509766\n",
      "Epoch 158, CIFAR-10 Batch 1:  Accuracy: 0.5242000222206116 Loss: 0.7842538952827454\n",
      "Epoch 158, CIFAR-10 Batch 2:  Accuracy: 0.5199999809265137 Loss: 1.0301183462142944\n",
      "Epoch 158, CIFAR-10 Batch 3:  Accuracy: 0.5246000289916992 Loss: 0.7035881280899048\n",
      "Epoch 158, CIFAR-10 Batch 4:  Accuracy: 0.5257999897003174 Loss: 0.7555590867996216\n",
      "Epoch 158, CIFAR-10 Batch 5:  Accuracy: 0.5253999829292297 Loss: 1.0081409215927124\n",
      "Epoch 159, CIFAR-10 Batch 1:  Accuracy: 0.524399995803833 Loss: 0.7443302869796753\n",
      "Epoch 159, CIFAR-10 Batch 2:  Accuracy: 0.5202000141143799 Loss: 1.0401042699813843\n",
      "Epoch 159, CIFAR-10 Batch 3:  Accuracy: 0.5242000222206116 Loss: 0.7143176794052124\n",
      "Epoch 159, CIFAR-10 Batch 4:  Accuracy: 0.5289999842643738 Loss: 0.7326406240463257\n",
      "Epoch 159, CIFAR-10 Batch 5:  Accuracy: 0.5321999788284302 Loss: 1.0024511814117432\n",
      "Epoch 160, CIFAR-10 Batch 1:  Accuracy: 0.5252000093460083 Loss: 0.7480541467666626\n",
      "Epoch 160, CIFAR-10 Batch 2:  Accuracy: 0.5246000289916992 Loss: 1.0336700677871704\n",
      "Epoch 160, CIFAR-10 Batch 3:  Accuracy: 0.5234000086784363 Loss: 0.6905199289321899\n",
      "Epoch 160, CIFAR-10 Batch 4:  Accuracy: 0.5253999829292297 Loss: 0.7429014444351196\n",
      "Epoch 160, CIFAR-10 Batch 5:  Accuracy: 0.5293999910354614 Loss: 1.0165789127349854\n",
      "Epoch 161, CIFAR-10 Batch 1:  Accuracy: 0.5206000208854675 Loss: 0.7649239897727966\n",
      "Epoch 161, CIFAR-10 Batch 2:  Accuracy: 0.5198000073432922 Loss: 1.038346290588379\n",
      "Epoch 161, CIFAR-10 Batch 3:  Accuracy: 0.5239999890327454 Loss: 0.7166134119033813\n",
      "Epoch 161, CIFAR-10 Batch 4:  Accuracy: 0.5224000215530396 Loss: 0.7759487628936768\n",
      "Epoch 161, CIFAR-10 Batch 5:  Accuracy: 0.5320000052452087 Loss: 1.0216779708862305\n",
      "Epoch 162, CIFAR-10 Batch 1:  Accuracy: 0.5194000005722046 Loss: 0.7600588798522949\n",
      "Epoch 162, CIFAR-10 Batch 2:  Accuracy: 0.5239999890327454 Loss: 1.011782169342041\n",
      "Epoch 162, CIFAR-10 Batch 3:  Accuracy: 0.521399974822998 Loss: 0.7113286256790161\n",
      "Epoch 162, CIFAR-10 Batch 4:  Accuracy: 0.5238000154495239 Loss: 0.7365182638168335\n",
      "Epoch 162, CIFAR-10 Batch 5:  Accuracy: 0.5257999897003174 Loss: 1.0230762958526611\n",
      "Epoch 163, CIFAR-10 Batch 1:  Accuracy: 0.5221999883651733 Loss: 0.7562912702560425\n",
      "Epoch 163, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 1.0233622789382935\n",
      "Epoch 163, CIFAR-10 Batch 3:  Accuracy: 0.5253999829292297 Loss: 0.7309974431991577\n",
      "Epoch 163, CIFAR-10 Batch 4:  Accuracy: 0.5248000025749207 Loss: 0.7619702219963074\n",
      "Epoch 163, CIFAR-10 Batch 5:  Accuracy: 0.5267999768257141 Loss: 0.9996198415756226\n",
      "Epoch 164, CIFAR-10 Batch 1:  Accuracy: 0.5216000080108643 Loss: 0.7483358383178711\n",
      "Epoch 164, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 1.0154633522033691\n",
      "Epoch 164, CIFAR-10 Batch 3:  Accuracy: 0.5289999842643738 Loss: 0.7471152544021606\n",
      "Epoch 164, CIFAR-10 Batch 4:  Accuracy: 0.5217999815940857 Loss: 0.7459667921066284\n",
      "Epoch 164, CIFAR-10 Batch 5:  Accuracy: 0.5299999713897705 Loss: 0.9789488911628723\n",
      "Epoch 165, CIFAR-10 Batch 1:  Accuracy: 0.5252000093460083 Loss: 0.7728317975997925\n",
      "Epoch 165, CIFAR-10 Batch 2:  Accuracy: 0.5228000283241272 Loss: 1.014737844467163\n",
      "Epoch 165, CIFAR-10 Batch 3:  Accuracy: 0.5212000012397766 Loss: 0.7169405221939087\n",
      "Epoch 165, CIFAR-10 Batch 4:  Accuracy: 0.524399995803833 Loss: 0.719096302986145\n",
      "Epoch 165, CIFAR-10 Batch 5:  Accuracy: 0.5246000289916992 Loss: 0.970980167388916\n",
      "Epoch 166, CIFAR-10 Batch 1:  Accuracy: 0.5311999917030334 Loss: 0.7530162334442139\n",
      "Epoch 166, CIFAR-10 Batch 2:  Accuracy: 0.5189999938011169 Loss: 1.0537223815917969\n",
      "Epoch 166, CIFAR-10 Batch 3:  Accuracy: 0.5189999938011169 Loss: 0.7057040929794312\n",
      "Epoch 166, CIFAR-10 Batch 4:  Accuracy: 0.5270000100135803 Loss: 0.7397664785385132\n",
      "Epoch 166, CIFAR-10 Batch 5:  Accuracy: 0.5332000255584717 Loss: 0.962807297706604\n",
      "Epoch 167, CIFAR-10 Batch 1:  Accuracy: 0.5278000235557556 Loss: 0.7542110681533813\n",
      "Epoch 167, CIFAR-10 Batch 2:  Accuracy: 0.5264000296592712 Loss: 1.0094555616378784\n",
      "Epoch 167, CIFAR-10 Batch 3:  Accuracy: 0.5285999774932861 Loss: 0.696306049823761\n",
      "Epoch 167, CIFAR-10 Batch 4:  Accuracy: 0.5246000289916992 Loss: 0.6948578953742981\n",
      "Epoch 167, CIFAR-10 Batch 5:  Accuracy: 0.5311999917030334 Loss: 0.9350591897964478\n",
      "Epoch 168, CIFAR-10 Batch 1:  Accuracy: 0.534600019454956 Loss: 0.7504103779792786\n",
      "Epoch 168, CIFAR-10 Batch 2:  Accuracy: 0.5230000019073486 Loss: 1.055955171585083\n",
      "Epoch 168, CIFAR-10 Batch 3:  Accuracy: 0.5189999938011169 Loss: 0.6805449724197388\n",
      "Epoch 168, CIFAR-10 Batch 4:  Accuracy: 0.5303999781608582 Loss: 0.7419601678848267\n",
      "Epoch 168, CIFAR-10 Batch 5:  Accuracy: 0.5289999842643738 Loss: 0.9934396743774414\n",
      "Epoch 169, CIFAR-10 Batch 1:  Accuracy: 0.526199996471405 Loss: 0.7498201131820679\n",
      "Epoch 169, CIFAR-10 Batch 2:  Accuracy: 0.5289999842643738 Loss: 1.003539800643921\n",
      "Epoch 169, CIFAR-10 Batch 3:  Accuracy: 0.5238000154495239 Loss: 0.687375545501709\n",
      "Epoch 169, CIFAR-10 Batch 4:  Accuracy: 0.5275999903678894 Loss: 0.7301974296569824\n",
      "Epoch 169, CIFAR-10 Batch 5:  Accuracy: 0.5281999707221985 Loss: 0.9738837480545044\n",
      "Epoch 170, CIFAR-10 Batch 1:  Accuracy: 0.5257999897003174 Loss: 0.7227418422698975\n",
      "Epoch 170, CIFAR-10 Batch 2:  Accuracy: 0.5266000032424927 Loss: 1.0429311990737915\n",
      "Epoch 170, CIFAR-10 Batch 3:  Accuracy: 0.5253999829292297 Loss: 0.695103645324707\n",
      "Epoch 170, CIFAR-10 Batch 4:  Accuracy: 0.5299999713897705 Loss: 0.7270394563674927\n",
      "Epoch 170, CIFAR-10 Batch 5:  Accuracy: 0.5281999707221985 Loss: 0.9722549319267273\n",
      "Epoch 171, CIFAR-10 Batch 1:  Accuracy: 0.5249999761581421 Loss: 0.7266379594802856\n",
      "Epoch 171, CIFAR-10 Batch 2:  Accuracy: 0.5194000005722046 Loss: 1.0388753414154053\n",
      "Epoch 171, CIFAR-10 Batch 3:  Accuracy: 0.5267999768257141 Loss: 0.6897308230400085\n",
      "Epoch 171, CIFAR-10 Batch 4:  Accuracy: 0.5302000045776367 Loss: 0.7041532397270203\n",
      "Epoch 171, CIFAR-10 Batch 5:  Accuracy: 0.5299999713897705 Loss: 0.9731687307357788\n",
      "Epoch 172, CIFAR-10 Batch 1:  Accuracy: 0.5284000039100647 Loss: 0.7361568212509155\n",
      "Epoch 172, CIFAR-10 Batch 2:  Accuracy: 0.5284000039100647 Loss: 1.0090253353118896\n",
      "Epoch 172, CIFAR-10 Batch 3:  Accuracy: 0.5231999754905701 Loss: 0.6907157897949219\n",
      "Epoch 172, CIFAR-10 Batch 4:  Accuracy: 0.5315999984741211 Loss: 0.6964980959892273\n",
      "Epoch 172, CIFAR-10 Batch 5:  Accuracy: 0.5288000106811523 Loss: 0.9852798581123352\n",
      "Epoch 173, CIFAR-10 Batch 1:  Accuracy: 0.5297999978065491 Loss: 0.7342321872711182\n",
      "Epoch 173, CIFAR-10 Batch 2:  Accuracy: 0.525600016117096 Loss: 1.0423294305801392\n",
      "Epoch 173, CIFAR-10 Batch 3:  Accuracy: 0.5220000147819519 Loss: 0.665870726108551\n",
      "Epoch 173, CIFAR-10 Batch 4:  Accuracy: 0.5296000242233276 Loss: 0.7052313089370728\n",
      "Epoch 173, CIFAR-10 Batch 5:  Accuracy: 0.5278000235557556 Loss: 0.9633750915527344\n",
      "Epoch 174, CIFAR-10 Batch 1:  Accuracy: 0.5235999822616577 Loss: 0.726009726524353\n",
      "Epoch 174, CIFAR-10 Batch 2:  Accuracy: 0.5198000073432922 Loss: 1.0172936916351318\n",
      "Epoch 174, CIFAR-10 Batch 3:  Accuracy: 0.5271999835968018 Loss: 0.7043001651763916\n",
      "Epoch 174, CIFAR-10 Batch 4:  Accuracy: 0.5284000039100647 Loss: 0.6897171139717102\n",
      "Epoch 174, CIFAR-10 Batch 5:  Accuracy: 0.5293999910354614 Loss: 0.9731286764144897\n",
      "Epoch 175, CIFAR-10 Batch 1:  Accuracy: 0.524399995803833 Loss: 0.7345982789993286\n",
      "Epoch 175, CIFAR-10 Batch 2:  Accuracy: 0.5253999829292297 Loss: 1.021958351135254\n",
      "Epoch 175, CIFAR-10 Batch 3:  Accuracy: 0.520799994468689 Loss: 0.7103765606880188\n",
      "Epoch 175, CIFAR-10 Batch 4:  Accuracy: 0.5297999978065491 Loss: 0.7321664094924927\n",
      "Epoch 175, CIFAR-10 Batch 5:  Accuracy: 0.5285999774932861 Loss: 0.9759836196899414\n",
      "Epoch 176, CIFAR-10 Batch 1:  Accuracy: 0.5253999829292297 Loss: 0.7293018102645874\n",
      "Epoch 176, CIFAR-10 Batch 2:  Accuracy: 0.5238000154495239 Loss: 1.0173757076263428\n",
      "Epoch 176, CIFAR-10 Batch 3:  Accuracy: 0.5260000228881836 Loss: 0.6661429405212402\n",
      "Epoch 176, CIFAR-10 Batch 4:  Accuracy: 0.5296000242233276 Loss: 0.692462146282196\n",
      "Epoch 176, CIFAR-10 Batch 5:  Accuracy: 0.5242000222206116 Loss: 1.053344488143921\n",
      "Epoch 177, CIFAR-10 Batch 1:  Accuracy: 0.5285999774932861 Loss: 0.7287964820861816\n",
      "Epoch 177, CIFAR-10 Batch 2:  Accuracy: 0.5270000100135803 Loss: 0.9886313676834106\n",
      "Epoch 177, CIFAR-10 Batch 3:  Accuracy: 0.5278000235557556 Loss: 0.6861234307289124\n",
      "Epoch 177, CIFAR-10 Batch 4:  Accuracy: 0.5260000228881836 Loss: 0.6876230239868164\n",
      "Epoch 177, CIFAR-10 Batch 5:  Accuracy: 0.52920001745224 Loss: 0.9641349911689758\n",
      "Epoch 178, CIFAR-10 Batch 1:  Accuracy: 0.5311999917030334 Loss: 0.7794798612594604\n",
      "Epoch 178, CIFAR-10 Batch 2:  Accuracy: 0.5203999876976013 Loss: 1.0041029453277588\n",
      "Epoch 178, CIFAR-10 Batch 3:  Accuracy: 0.525600016117096 Loss: 0.6881040334701538\n",
      "Epoch 178, CIFAR-10 Batch 4:  Accuracy: 0.527400016784668 Loss: 0.6895879507064819\n",
      "Epoch 178, CIFAR-10 Batch 5:  Accuracy: 0.5315999984741211 Loss: 0.9576930999755859\n",
      "Epoch 179, CIFAR-10 Batch 1:  Accuracy: 0.5307999849319458 Loss: 0.7333333492279053\n",
      "Epoch 179, CIFAR-10 Batch 2:  Accuracy: 0.5246000289916992 Loss: 0.9935725927352905\n",
      "Epoch 179, CIFAR-10 Batch 3:  Accuracy: 0.5239999890327454 Loss: 0.6656083464622498\n",
      "Epoch 179, CIFAR-10 Batch 4:  Accuracy: 0.5314000248908997 Loss: 0.7121397852897644\n",
      "Epoch 179, CIFAR-10 Batch 5:  Accuracy: 0.5302000045776367 Loss: 0.9402860403060913\n",
      "Epoch 180, CIFAR-10 Batch 1:  Accuracy: 0.5271999835968018 Loss: 0.7340978980064392\n",
      "Epoch 180, CIFAR-10 Batch 2:  Accuracy: 0.5224000215530396 Loss: 0.9835710525512695\n",
      "Epoch 180, CIFAR-10 Batch 3:  Accuracy: 0.5192000269889832 Loss: 0.6813545227050781\n",
      "Epoch 180, CIFAR-10 Batch 4:  Accuracy: 0.5260000228881836 Loss: 0.6767374277114868\n",
      "Epoch 180, CIFAR-10 Batch 5:  Accuracy: 0.52920001745224 Loss: 0.9419910311698914\n",
      "Epoch 181, CIFAR-10 Batch 1:  Accuracy: 0.5249999761581421 Loss: 0.7893357872962952\n",
      "Epoch 181, CIFAR-10 Batch 2:  Accuracy: 0.5203999876976013 Loss: 0.9822858572006226\n",
      "Epoch 181, CIFAR-10 Batch 3:  Accuracy: 0.5194000005722046 Loss: 0.6954464912414551\n",
      "Epoch 181, CIFAR-10 Batch 4:  Accuracy: 0.5281999707221985 Loss: 0.69228595495224\n",
      "Epoch 181, CIFAR-10 Batch 5:  Accuracy: 0.526199996471405 Loss: 0.9448195695877075\n",
      "Epoch 182, CIFAR-10 Batch 1:  Accuracy: 0.5210000276565552 Loss: 0.7187088131904602\n",
      "Epoch 182, CIFAR-10 Batch 2:  Accuracy: 0.5275999903678894 Loss: 1.0020129680633545\n",
      "Epoch 182, CIFAR-10 Batch 3:  Accuracy: 0.5231999754905701 Loss: 0.6926981210708618\n",
      "Epoch 182, CIFAR-10 Batch 4:  Accuracy: 0.5293999910354614 Loss: 0.6868381500244141\n",
      "Epoch 182, CIFAR-10 Batch 5:  Accuracy: 0.5239999890327454 Loss: 0.9499584436416626\n",
      "Epoch 183, CIFAR-10 Batch 1:  Accuracy: 0.5288000106811523 Loss: 0.7851496338844299\n",
      "Epoch 183, CIFAR-10 Batch 2:  Accuracy: 0.5252000093460083 Loss: 0.9839639663696289\n",
      "Epoch 183, CIFAR-10 Batch 3:  Accuracy: 0.5231999754905701 Loss: 0.6808440685272217\n",
      "Epoch 183, CIFAR-10 Batch 4:  Accuracy: 0.527400016784668 Loss: 0.6620020866394043\n",
      "Epoch 183, CIFAR-10 Batch 5:  Accuracy: 0.527999997138977 Loss: 0.9098137021064758\n",
      "Epoch 184, CIFAR-10 Batch 1:  Accuracy: 0.5284000039100647 Loss: 0.7819495797157288\n",
      "Epoch 184, CIFAR-10 Batch 2:  Accuracy: 0.5264000296592712 Loss: 0.9772909879684448\n",
      "Epoch 184, CIFAR-10 Batch 3:  Accuracy: 0.5234000086784363 Loss: 0.6581278443336487\n",
      "Epoch 184, CIFAR-10 Batch 4:  Accuracy: 0.5288000106811523 Loss: 0.6583949327468872\n",
      "Epoch 184, CIFAR-10 Batch 5:  Accuracy: 0.524399995803833 Loss: 0.9497709274291992\n",
      "Epoch 185, CIFAR-10 Batch 1:  Accuracy: 0.5238000154495239 Loss: 0.7247627973556519\n",
      "Epoch 185, CIFAR-10 Batch 2:  Accuracy: 0.5293999910354614 Loss: 0.971780002117157\n",
      "Epoch 185, CIFAR-10 Batch 3:  Accuracy: 0.5228000283241272 Loss: 0.67550128698349\n",
      "Epoch 185, CIFAR-10 Batch 4:  Accuracy: 0.5281999707221985 Loss: 0.6475825309753418\n",
      "Epoch 185, CIFAR-10 Batch 5:  Accuracy: 0.5299999713897705 Loss: 0.9344371557235718\n",
      "Epoch 186, CIFAR-10 Batch 1:  Accuracy: 0.5307999849319458 Loss: 0.7514818906784058\n",
      "Epoch 186, CIFAR-10 Batch 2:  Accuracy: 0.5242000222206116 Loss: 1.0019458532333374\n",
      "Epoch 186, CIFAR-10 Batch 3:  Accuracy: 0.5249999761581421 Loss: 0.691970944404602\n",
      "Epoch 186, CIFAR-10 Batch 4:  Accuracy: 0.5293999910354614 Loss: 0.680740237236023\n",
      "Epoch 186, CIFAR-10 Batch 5:  Accuracy: 0.5281999707221985 Loss: 0.8853654861450195\n",
      "Epoch 187, CIFAR-10 Batch 1:  Accuracy: 0.5289999842643738 Loss: 0.8045644760131836\n",
      "Epoch 187, CIFAR-10 Batch 2:  Accuracy: 0.5212000012397766 Loss: 0.9658054113388062\n",
      "Epoch 187, CIFAR-10 Batch 3:  Accuracy: 0.5270000100135803 Loss: 0.6776319146156311\n",
      "Epoch 187, CIFAR-10 Batch 4:  Accuracy: 0.527999997138977 Loss: 0.7013587951660156\n",
      "Epoch 187, CIFAR-10 Batch 5:  Accuracy: 0.5302000045776367 Loss: 0.9104359745979309\n",
      "Epoch 188, CIFAR-10 Batch 1:  Accuracy: 0.5320000052452087 Loss: 0.7234400510787964\n",
      "Epoch 188, CIFAR-10 Batch 2:  Accuracy: 0.5260000228881836 Loss: 0.9889386892318726\n",
      "Epoch 188, CIFAR-10 Batch 3:  Accuracy: 0.5257999897003174 Loss: 0.6966534852981567\n",
      "Epoch 188, CIFAR-10 Batch 4:  Accuracy: 0.5239999890327454 Loss: 0.6760753393173218\n",
      "Epoch 188, CIFAR-10 Batch 5:  Accuracy: 0.5185999870300293 Loss: 0.9418591260910034\n",
      "Epoch 189, CIFAR-10 Batch 1:  Accuracy: 0.5289999842643738 Loss: 0.7519716024398804\n",
      "Epoch 189, CIFAR-10 Batch 2:  Accuracy: 0.527999997138977 Loss: 0.9490659832954407\n",
      "Epoch 189, CIFAR-10 Batch 3:  Accuracy: 0.5212000012397766 Loss: 0.6897018551826477\n",
      "Epoch 189, CIFAR-10 Batch 4:  Accuracy: 0.526199996471405 Loss: 0.6981147527694702\n",
      "Epoch 189, CIFAR-10 Batch 5:  Accuracy: 0.5252000093460083 Loss: 0.9203135371208191\n",
      "Epoch 190, CIFAR-10 Batch 1:  Accuracy: 0.5284000039100647 Loss: 0.7486490607261658\n",
      "Epoch 190, CIFAR-10 Batch 2:  Accuracy: 0.5238000154495239 Loss: 0.9488294720649719\n",
      "Epoch 190, CIFAR-10 Batch 3:  Accuracy: 0.526199996471405 Loss: 0.6570364236831665\n",
      "Epoch 190, CIFAR-10 Batch 4:  Accuracy: 0.5267999768257141 Loss: 0.6724683046340942\n",
      "Epoch 190, CIFAR-10 Batch 5:  Accuracy: 0.5271999835968018 Loss: 0.9442406892776489\n",
      "Epoch 191, CIFAR-10 Batch 1:  Accuracy: 0.526199996471405 Loss: 0.7377685308456421\n",
      "Epoch 191, CIFAR-10 Batch 2:  Accuracy: 0.5185999870300293 Loss: 0.9628133773803711\n",
      "Epoch 191, CIFAR-10 Batch 3:  Accuracy: 0.527999997138977 Loss: 0.6513580083847046\n",
      "Epoch 191, CIFAR-10 Batch 4:  Accuracy: 0.5353999733924866 Loss: 0.7109025716781616\n",
      "Epoch 191, CIFAR-10 Batch 5:  Accuracy: 0.5275999903678894 Loss: 0.9949140548706055\n",
      "Epoch 192, CIFAR-10 Batch 1:  Accuracy: 0.5278000235557556 Loss: 0.7110332250595093\n",
      "Epoch 192, CIFAR-10 Batch 2:  Accuracy: 0.5181999802589417 Loss: 0.973268985748291\n",
      "Epoch 192, CIFAR-10 Batch 3:  Accuracy: 0.527400016784668 Loss: 0.6707622408866882\n",
      "Epoch 192, CIFAR-10 Batch 4:  Accuracy: 0.5206000208854675 Loss: 0.7240168452262878\n",
      "Epoch 192, CIFAR-10 Batch 5:  Accuracy: 0.526199996471405 Loss: 0.9532782435417175\n",
      "Epoch 193, CIFAR-10 Batch 1:  Accuracy: 0.5270000100135803 Loss: 0.7294910550117493\n",
      "Epoch 193, CIFAR-10 Batch 2:  Accuracy: 0.5231999754905701 Loss: 0.9766520261764526\n",
      "Epoch 193, CIFAR-10 Batch 3:  Accuracy: 0.5275999903678894 Loss: 0.6880782842636108\n",
      "Epoch 193, CIFAR-10 Batch 4:  Accuracy: 0.525600016117096 Loss: 0.638283371925354\n",
      "Epoch 193, CIFAR-10 Batch 5:  Accuracy: 0.525600016117096 Loss: 0.9139531850814819\n",
      "Epoch 194, CIFAR-10 Batch 1:  Accuracy: 0.5246000289916992 Loss: 0.6929162740707397\n",
      "Epoch 194, CIFAR-10 Batch 2:  Accuracy: 0.5216000080108643 Loss: 0.9770753979682922\n",
      "Epoch 194, CIFAR-10 Batch 3:  Accuracy: 0.5246000289916992 Loss: 0.6518016457557678\n",
      "Epoch 194, CIFAR-10 Batch 4:  Accuracy: 0.5288000106811523 Loss: 0.6948127746582031\n",
      "Epoch 194, CIFAR-10 Batch 5:  Accuracy: 0.5238000154495239 Loss: 0.900944709777832\n",
      "Epoch 195, CIFAR-10 Batch 1:  Accuracy: 0.5275999903678894 Loss: 0.7372723817825317\n",
      "Epoch 195, CIFAR-10 Batch 2:  Accuracy: 0.5297999978065491 Loss: 0.9730884432792664\n",
      "Epoch 195, CIFAR-10 Batch 3:  Accuracy: 0.526199996471405 Loss: 0.6790030598640442\n",
      "Epoch 195, CIFAR-10 Batch 4:  Accuracy: 0.5248000025749207 Loss: 0.6747787594795227\n",
      "Epoch 195, CIFAR-10 Batch 5:  Accuracy: 0.5284000039100647 Loss: 0.9170140027999878\n",
      "Epoch 196, CIFAR-10 Batch 1:  Accuracy: 0.5271999835968018 Loss: 0.727144718170166\n",
      "Epoch 196, CIFAR-10 Batch 2:  Accuracy: 0.5231999754905701 Loss: 0.988148033618927\n",
      "Epoch 196, CIFAR-10 Batch 3:  Accuracy: 0.522599995136261 Loss: 0.6612709760665894\n",
      "Epoch 196, CIFAR-10 Batch 4:  Accuracy: 0.5297999978065491 Loss: 0.6606621742248535\n",
      "Epoch 196, CIFAR-10 Batch 5:  Accuracy: 0.5253999829292297 Loss: 0.9276444315910339\n",
      "Epoch 197, CIFAR-10 Batch 1:  Accuracy: 0.5257999897003174 Loss: 0.7114311456680298\n",
      "Epoch 197, CIFAR-10 Batch 2:  Accuracy: 0.525600016117096 Loss: 0.9663165211677551\n",
      "Epoch 197, CIFAR-10 Batch 3:  Accuracy: 0.5257999897003174 Loss: 0.6882016062736511\n",
      "Epoch 197, CIFAR-10 Batch 4:  Accuracy: 0.527400016784668 Loss: 0.6765670776367188\n",
      "Epoch 197, CIFAR-10 Batch 5:  Accuracy: 0.5285999774932861 Loss: 0.9172452688217163\n",
      "Epoch 198, CIFAR-10 Batch 1:  Accuracy: 0.524399995803833 Loss: 0.7212535738945007\n",
      "Epoch 198, CIFAR-10 Batch 2:  Accuracy: 0.5285999774932861 Loss: 0.9310528635978699\n",
      "Epoch 198, CIFAR-10 Batch 3:  Accuracy: 0.520799994468689 Loss: 0.6561874151229858\n",
      "Epoch 198, CIFAR-10 Batch 4:  Accuracy: 0.525600016117096 Loss: 0.6639415621757507\n",
      "Epoch 198, CIFAR-10 Batch 5:  Accuracy: 0.5224000215530396 Loss: 0.9182230234146118\n",
      "Epoch 199, CIFAR-10 Batch 1:  Accuracy: 0.5238000154495239 Loss: 0.6956437826156616\n",
      "Epoch 199, CIFAR-10 Batch 2:  Accuracy: 0.5267999768257141 Loss: 0.9413732290267944\n",
      "Epoch 199, CIFAR-10 Batch 3:  Accuracy: 0.5239999890327454 Loss: 0.6541295647621155\n",
      "Epoch 199, CIFAR-10 Batch 4:  Accuracy: 0.5271999835968018 Loss: 0.6812998652458191\n",
      "Epoch 199, CIFAR-10 Batch 5:  Accuracy: 0.5216000080108643 Loss: 0.8760473132133484\n",
      "Epoch 200, CIFAR-10 Batch 1:  Accuracy: 0.5249999761581421 Loss: 0.6746145486831665\n",
      "Epoch 200, CIFAR-10 Batch 2:  Accuracy: 0.5260000228881836 Loss: 0.9472455978393555\n",
      "Epoch 200, CIFAR-10 Batch 3:  Accuracy: 0.5235999822616577 Loss: 0.65715491771698\n",
      "Epoch 200, CIFAR-10 Batch 4:  Accuracy: 0.5285999774932861 Loss: 0.6451359391212463\n",
      "Epoch 200, CIFAR-10 Batch 5:  Accuracy: 0.5260000228881836 Loss: 0.9138663411140442\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.529568829113924\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117aa30f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
